{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /home/coraline/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import benepar, spacy\n",
    "import nltk\n",
    "benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/coraline/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ca67f2b6f0458584f954110789575d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_dataset = datasets.load_dataset(\"imdb\", split=['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8dcb152e2433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimdb_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "imdb_dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<benepar.integrations.spacy_plugin.BeneparComponent at 0x7f0217f91890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coraline/anaconda3/envs/bert-attack/lib/python3.7/site-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  'with `validate_args=False` to turn off validation.')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('The time for action is now. It is never too late to do something.')\n",
    "sent = list(doc.sents)[0]\n",
    "print(sent._.parse_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('S',)\n"
     ]
    }
   ],
   "source": [
    "print(sent._.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for action\n"
     ]
    }
   ],
   "source": [
    "print(list(sent._.children)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "                           |                     \n",
      "                           S                    \n",
      "               ____________|__________________   \n",
      "              NP                     |        | \n",
      "      ________|_______               |        |  \n",
      "     |                PP             VP       | \n",
      "     |             ___|____       ___|___     |  \n",
      "     NP           |        NP    |      ADVP  | \n",
      "  ___|___         |        |     |       |    |  \n",
      " DT      NN       IN       NN   VBZ      RB   . \n",
      " |       |        |        |     |       |    |  \n",
      "The     time     for     action  is     now   . \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import Tree, ParentedTree\n",
    "parse_tree = ParentedTree.fromstring('(' + sent._.parse_string + ')')\n",
    "print(parse_tree.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da24ad4dba546b2b5e5d4d99f97f650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 01:36:36 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f71e515326743e79a22fe53a9c1d32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://huggingface.co/stanfordnlp/stanza-en/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 01:37:25 INFO: Finished downloading models and saved to /home/coraline/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en') # download English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 01:56:44 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2022-03-02 01:56:44 INFO: Use device: gpu\n",
      "2022-03-02 01:56:44 INFO: Loading: tokenize\n",
      "2022-03-02 01:56:44 INFO: Loading: pos\n",
      "2022-03-02 01:56:44 INFO: Loading: constituency\n",
      "2022-03-02 01:56:44 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT a) (NN test)))))\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')\n",
    "doc = nlp('This is a test')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)\n",
    "    const_tree = str(sentence.constituency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ROOT             \n",
      "          |                \n",
      "          S               \n",
      "  ________|____            \n",
      " |             VP         \n",
      " |     ________|___        \n",
      " NP   |            NP     \n",
      " |    |         ___|___    \n",
      " DT  VBZ       DT      NN \n",
      " |    |        |       |   \n",
      "This  is       a      test\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import Tree, ParentedTree\n",
    "\n",
    "parse_tree = ParentedTree.fromstring(const_tree)\n",
    "print(parse_tree.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROOT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = doc.sentences[0].constituency\n",
    "tree.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(S (NP (DT This)) (VP (VBZ is) (NP (DT a) (NN test))))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(NP (DT This)), (VP (VBZ is) (NP (DT a) (NN test)))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.children[0].children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCFG\n",
    "Extract Grammar from Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/coraline/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "from nltk import treetransforms\n",
    "from nltk import induce_pcfg\n",
    "from nltk.parse import pchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Induce PCFG grammar from treebank data:\n"
     ]
    }
   ],
   "source": [
    "# extract productions from three trees and induce the PCFG\n",
    "print(\"Induce PCFG grammar from treebank data:\")\n",
    "\n",
    "productions = []\n",
    "for item in treebank.fileids()[:2]:\n",
    "    for tree in treebank.parsed_sents(item):\n",
    "        # perform optional tree transformations, e.g.:\n",
    "        tree.collapse_unary(collapsePOS = False)# Remove branches A-B-C into A-B+C\n",
    "        tree.chomsky_normal_form(horzMarkov = 2)# Remove A->(B,C,D) into A->B,C+D->D\n",
    "        productions += tree.productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 86 productions (start state = S)\n",
      "    S -> NP-SBJ S|<VP-.> [0.5]\n",
      "    NP-SBJ -> NP NP-SBJ|<,-ADJP> [0.333333]\n",
      "    NP -> NNP NNP [0.2]\n",
      "    NNP -> 'Pierre' [0.0714286]\n",
      "    NNP -> 'Vinken' [0.142857]\n",
      "    NP-SBJ|<,-ADJP> -> , NP-SBJ|<ADJP-,> [1.0]\n",
      "    , -> ',' [1.0]\n",
      "    NP-SBJ|<ADJP-,> -> ADJP , [1.0]\n",
      "    ADJP -> NP JJ [1.0]\n",
      "    NP -> CD NNS [0.133333]\n",
      "    CD -> '61' [0.333333]\n",
      "    NNS -> 'years' [1.0]\n",
      "    JJ -> 'old' [0.285714]\n",
      "    S|<VP-.> -> VP . [1.0]\n",
      "    VP -> MD VP [0.2]\n",
      "    MD -> 'will' [1.0]\n",
      "    VP -> VB VP|<NP-PP-CLR> [0.2]\n",
      "    VB -> 'join' [1.0]\n",
      "    VP|<NP-PP-CLR> -> NP VP|<PP-CLR-NP-TMP> [1.0]\n",
      "    NP -> DT NN [0.0666667]\n",
      "    DT -> 'the' [0.4]\n",
      "    NN -> 'board' [0.142857]\n",
      "    VP|<PP-CLR-NP-TMP> -> PP-CLR NP-TMP [1.0]\n",
      "    PP-CLR -> IN NP [1.0]\n",
      "    IN -> 'as' [0.25]\n",
      "    NP -> DT NP|<JJ-NN> [0.133333]\n",
      "    DT -> 'a' [0.4]\n",
      "    NP|<JJ-NN> -> JJ NN [1.0]\n",
      "    JJ -> 'nonexecutive' [0.285714]\n",
      "    NN -> 'director' [0.285714]\n",
      "    NP-TMP -> NNP CD [1.0]\n",
      "    NNP -> 'Nov.' [0.0714286]\n",
      "    CD -> '29' [0.333333]\n",
      "    . -> '.' [1.0]\n",
      "    NP-SBJ -> NNP NNP [0.333333]\n",
      "    NNP -> 'Mr.' [0.0714286]\n",
      "    VP -> VBZ NP-PRD [0.2]\n",
      "    VBZ -> 'is' [1.0]\n",
      "    NP-PRD -> NP PP [1.0]\n",
      "    NP -> NN [0.0666667]\n",
      "    NN -> 'chairman' [0.285714]\n",
      "    PP -> IN NP [1.0]\n",
      "    IN -> 'of' [0.75]\n",
      "    NP -> NP NP|<,-NP> [0.0666667]\n",
      "    NNP -> 'Elsevier' [0.0714286]\n",
      "    NNP -> 'N.V.' [0.0714286]\n",
      "    NP|<,-NP> -> , NP [1.0]\n",
      "    NP -> DT NP|<NNP-VBG> [0.0666667]\n",
      "    NP|<NNP-VBG> -> NNP NP|<VBG-NN> [1.0]\n",
      "    NNP -> 'Dutch' [0.0714286]\n",
      "    NP|<VBG-NN> -> VBG NN [1.0]\n",
      "    VBG -> 'publishing' [1.0]\n",
      "    NN -> 'group' [0.142857]\n",
      "    S -> NP-SBJ-1 S|<VP-.> [0.25]\n",
      "    NP-SBJ-1 -> NP NP-SBJ-1|<,-UCP> [1.0]\n",
      "    NNP -> 'Rudolph' [0.0714286]\n",
      "    NNP -> 'Agnew' [0.0714286]\n",
      "    NP-SBJ-1|<,-UCP> -> , NP-SBJ-1|<UCP-,> [1.0]\n",
      "    NP-SBJ-1|<UCP-,> -> UCP , [1.0]\n",
      "    UCP -> ADJP UCP|<CC-NP> [1.0]\n",
      "    CD -> '55' [0.333333]\n",
      "    UCP|<CC-NP> -> CC NP [1.0]\n",
      "    CC -> 'and' [1.0]\n",
      "    NP -> NP PP [0.0666667]\n",
      "    NP -> JJ NN [0.0666667]\n",
      "    JJ -> 'former' [0.142857]\n",
      "    NP -> NNP NP|<NNP-NNP> [0.0666667]\n",
      "    NNP -> 'Consolidated' [0.0714286]\n",
      "    NP|<NNP-NNP> -> NNP NP|<NNP-NNP> [0.5]\n",
      "    NNP -> 'Gold' [0.0714286]\n",
      "    NP|<NNP-NNP> -> NNP NNP [0.5]\n",
      "    NNP -> 'Fields' [0.0714286]\n",
      "    NNP -> 'PLC' [0.0714286]\n",
      "    VP -> VBD VP [0.2]\n",
      "    VBD -> 'was' [1.0]\n",
      "    VP -> VBN S [0.2]\n",
      "    VBN -> 'named' [1.0]\n",
      "    S -> NP-SBJ NP-PRD [0.25]\n",
      "    NP-SBJ -> -NONE- [0.333333]\n",
      "    -NONE- -> '*-1' [1.0]\n",
      "    NP -> DT NP|<JJ-JJ> [0.0666667]\n",
      "    DT -> 'this' [0.2]\n",
      "    NP|<JJ-JJ> -> JJ NP|<JJ-NN> [1.0]\n",
      "    JJ -> 'British' [0.142857]\n",
      "    JJ -> 'industrial' [0.142857]\n",
      "    NN -> 'conglomerate' [0.142857]\n"
     ]
    }
   ],
   "source": [
    "from nltk import Nonterminal\n",
    "S = Nonterminal('S')\n",
    "grammar = induce_pcfg(S, productions)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /home/coraline/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import benepar, spacy\n",
    "import nltk\n",
    "benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRLComponent:\n",
    "\n",
    "    def __init__(self, nlp: Language, model_path: str):\n",
    "        if not Doc.has_extension(\"srl\"):\n",
    "            Doc.set_extension(\"srl\", default=None)\n",
    "        \n",
    "        self.predictor = Predictor.from_path(model_path)\n",
    "\n",
    "    def __call__(self, doc: Doc):\n",
    "        predictions = self.predictor.predict(sentence=doc.text)\n",
    "        doc._.srl = predictions\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"srl\", default_config={\n",
    "    \"model_path\": \"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\"})\n",
    "def create_srl_component(nlp: Language, name: str, model_path: str):\n",
    "    return SRLComponent(nlp, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coraline/anaconda3/envs/bert-attack/lib/python3.7/site-packages/spacy/util.py:1504: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n",
      "2022-03-02 06:11:08,869 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2022-03-02 06:11:09,272 - INFO - cached_path - cache of https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz is up-to-date\n",
      "2022-03-02 06:11:09,272 - INFO - allennlp.models.archival - loading archive file https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz from cache at /home/coraline/.allennlp/cache/b5f1db011cc85691a5fa2bf29e055a712261a2e5d74a74edd7da2fffc98d4ab8.4c4ac7e06ec3d85631bd26b839f90b5a375d3ceeb43e3c74f1cf4758dcee2bb3\n",
      "2022-03-02 06:11:09,273 - INFO - allennlp.models.archival - extracting archive file /home/coraline/.allennlp/cache/b5f1db011cc85691a5fa2bf29e055a712261a2e5d74a74edd7da2fffc98d4ab8.4c4ac7e06ec3d85631bd26b839f90b5a375d3ceeb43e3c74f1cf4758dcee2bb3 to temp dir /tmp/tmp9uola044\n",
      "2022-03-02 06:11:11,845 - INFO - allennlp.common.params - dataset_reader.type = srl\n",
      "2022-03-02 06:11:11,846 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2022-03-02 06:11:11,846 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2022-03-02 06:11:11,847 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
      "2022-03-02 06:11:11,847 - INFO - allennlp.common.params - dataset_reader.token_indexers = None\n",
      "2022-03-02 06:11:11,847 - INFO - allennlp.common.params - dataset_reader.domain_identifier = None\n",
      "2022-03-02 06:11:11,848 - INFO - allennlp.common.params - dataset_reader.bert_model_name = bert-base-uncased\n",
      "2022-03-02 06:11:16,993 - INFO - allennlp.common.params - dataset_reader.type = srl\n",
      "2022-03-02 06:11:16,994 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2022-03-02 06:11:16,994 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2022-03-02 06:11:16,995 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
      "2022-03-02 06:11:16,995 - INFO - allennlp.common.params - dataset_reader.token_indexers = None\n",
      "2022-03-02 06:11:16,995 - INFO - allennlp.common.params - dataset_reader.domain_identifier = None\n",
      "2022-03-02 06:11:16,996 - INFO - allennlp.common.params - dataset_reader.bert_model_name = bert-base-uncased\n",
      "2022-03-02 06:11:22,658 - INFO - allennlp.common.params - type = from_instances\n",
      "2022-03-02 06:11:22,659 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmp9uola044/vocabulary.\n",
      "2022-03-02 06:11:22,660 - INFO - allennlp.common.params - model.type = srl_bert\n",
      "2022-03-02 06:11:22,660 - INFO - allennlp.common.params - model.regularizer = None\n",
      "2022-03-02 06:11:22,661 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
      "2022-03-02 06:11:22,661 - INFO - allennlp.common.params - model.bert_model = bert-base-uncased\n",
      "2022-03-02 06:11:22,661 - INFO - allennlp.common.params - model.embedding_dropout = 0.1\n",
      "2022-03-02 06:11:22,662 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f18ec3dc710>\n",
      "2022-03-02 06:11:22,662 - INFO - allennlp.common.params - model.label_smoothing = None\n",
      "2022-03-02 06:11:22,662 - INFO - allennlp.common.params - model.ignore_span_metric = False\n",
      "2022-03-02 06:11:22,662 - INFO - allennlp.common.params - model.srl_eval_path = /home/coraline/anaconda3/envs/bert-attack/lib/python3.7/site-packages/allennlp_models/structured_prediction/tools/srl-eval.pl\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-03-02 06:11:25,548 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2022-03-02 06:11:25,549 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2022-03-02 06:11:25,549 - INFO - allennlp.nn.initializers -    bert_model.embeddings.LayerNorm.bias\n",
      "2022-03-02 06:11:25,549 - INFO - allennlp.nn.initializers -    bert_model.embeddings.LayerNorm.weight\n",
      "2022-03-02 06:11:25,550 - INFO - allennlp.nn.initializers -    bert_model.embeddings.position_embeddings.weight\n",
      "2022-03-02 06:11:25,550 - INFO - allennlp.nn.initializers -    bert_model.embeddings.token_type_embeddings.weight\n",
      "2022-03-02 06:11:25,550 - INFO - allennlp.nn.initializers -    bert_model.embeddings.word_embeddings.weight\n",
      "2022-03-02 06:11:25,551 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,551 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,551 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,552 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,552 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.key.bias\n",
      "2022-03-02 06:11:25,552 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.key.weight\n",
      "2022-03-02 06:11:25,552 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.query.bias\n",
      "2022-03-02 06:11:25,552 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.query.weight\n",
      "2022-03-02 06:11:25,553 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.value.bias\n",
      "2022-03-02 06:11:25,553 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.value.weight\n",
      "2022-03-02 06:11:25,553 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,553 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,554 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,555 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,555 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.dense.bias\n",
      "2022-03-02 06:11:25,555 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.dense.weight\n",
      "2022-03-02 06:11:25,556 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,556 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,557 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,557 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,557 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.key.bias\n",
      "2022-03-02 06:11:25,558 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.key.weight\n",
      "2022-03-02 06:11:25,558 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.query.bias\n",
      "2022-03-02 06:11:25,558 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.query.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 06:11:25,558 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.value.bias\n",
      "2022-03-02 06:11:25,559 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.value.weight\n",
      "2022-03-02 06:11:25,559 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,559 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,559 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,560 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,560 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.dense.bias\n",
      "2022-03-02 06:11:25,560 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.dense.weight\n",
      "2022-03-02 06:11:25,560 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,560 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,561 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,562 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,562 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.key.bias\n",
      "2022-03-02 06:11:25,563 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.key.weight\n",
      "2022-03-02 06:11:25,563 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.query.bias\n",
      "2022-03-02 06:11:25,563 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.query.weight\n",
      "2022-03-02 06:11:25,564 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.value.bias\n",
      "2022-03-02 06:11:25,564 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.value.weight\n",
      "2022-03-02 06:11:25,565 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,565 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,566 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,566 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,566 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.dense.bias\n",
      "2022-03-02 06:11:25,567 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.dense.weight\n",
      "2022-03-02 06:11:25,567 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,567 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,568 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,568 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,568 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.key.bias\n",
      "2022-03-02 06:11:25,568 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.key.weight\n",
      "2022-03-02 06:11:25,569 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.query.bias\n",
      "2022-03-02 06:11:25,570 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.query.weight\n",
      "2022-03-02 06:11:25,570 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.value.bias\n",
      "2022-03-02 06:11:25,570 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.value.weight\n",
      "2022-03-02 06:11:25,570 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,571 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,571 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,571 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,572 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.dense.bias\n",
      "2022-03-02 06:11:25,572 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.dense.weight\n",
      "2022-03-02 06:11:25,573 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,573 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,574 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,574 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,574 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.key.bias\n",
      "2022-03-02 06:11:25,574 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.key.weight\n",
      "2022-03-02 06:11:25,575 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.query.bias\n",
      "2022-03-02 06:11:25,575 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.query.weight\n",
      "2022-03-02 06:11:25,575 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.value.bias\n",
      "2022-03-02 06:11:25,575 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.value.weight\n",
      "2022-03-02 06:11:25,576 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,576 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,576 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,576 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,577 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.dense.bias\n",
      "2022-03-02 06:11:25,577 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.dense.weight\n",
      "2022-03-02 06:11:25,577 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,577 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,578 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,578 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,578 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.key.bias\n",
      "2022-03-02 06:11:25,578 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.key.weight\n",
      "2022-03-02 06:11:25,579 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.query.bias\n",
      "2022-03-02 06:11:25,579 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.query.weight\n",
      "2022-03-02 06:11:25,579 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.value.bias\n",
      "2022-03-02 06:11:25,579 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.value.weight\n",
      "2022-03-02 06:11:25,579 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,580 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,580 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,580 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,580 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 06:11:25,581 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.dense.weight\n",
      "2022-03-02 06:11:25,581 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,583 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,583 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,584 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,584 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.key.bias\n",
      "2022-03-02 06:11:25,584 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.key.weight\n",
      "2022-03-02 06:11:25,584 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.query.bias\n",
      "2022-03-02 06:11:25,585 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.query.weight\n",
      "2022-03-02 06:11:25,585 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.value.bias\n",
      "2022-03-02 06:11:25,585 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.value.weight\n",
      "2022-03-02 06:11:25,585 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,586 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,586 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,586 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,587 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.dense.bias\n",
      "2022-03-02 06:11:25,587 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.dense.weight\n",
      "2022-03-02 06:11:25,587 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,587 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,588 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,588 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,588 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.key.bias\n",
      "2022-03-02 06:11:25,589 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.key.weight\n",
      "2022-03-02 06:11:25,589 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.query.bias\n",
      "2022-03-02 06:11:25,589 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.query.weight\n",
      "2022-03-02 06:11:25,589 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.value.bias\n",
      "2022-03-02 06:11:25,590 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.value.weight\n",
      "2022-03-02 06:11:25,590 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,590 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,590 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,591 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,591 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.dense.bias\n",
      "2022-03-02 06:11:25,591 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.dense.weight\n",
      "2022-03-02 06:11:25,591 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,592 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,592 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,592 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,593 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.key.bias\n",
      "2022-03-02 06:11:25,593 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.key.weight\n",
      "2022-03-02 06:11:25,593 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.query.bias\n",
      "2022-03-02 06:11:25,593 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.query.weight\n",
      "2022-03-02 06:11:25,594 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.value.bias\n",
      "2022-03-02 06:11:25,597 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.value.weight\n",
      "2022-03-02 06:11:25,597 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,597 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,598 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,598 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,598 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.dense.bias\n",
      "2022-03-02 06:11:25,598 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.dense.weight\n",
      "2022-03-02 06:11:25,599 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,599 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,600 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,600 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,600 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.key.bias\n",
      "2022-03-02 06:11:25,601 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.key.weight\n",
      "2022-03-02 06:11:25,601 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.query.bias\n",
      "2022-03-02 06:11:25,601 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.query.weight\n",
      "2022-03-02 06:11:25,601 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.value.bias\n",
      "2022-03-02 06:11:25,602 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.value.weight\n",
      "2022-03-02 06:11:25,602 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,602 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,602 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,603 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,603 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.dense.bias\n",
      "2022-03-02 06:11:25,603 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.dense.weight\n",
      "2022-03-02 06:11:25,603 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,604 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,604 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,604 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,605 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.key.bias\n",
      "2022-03-02 06:11:25,605 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 06:11:25,605 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.query.bias\n",
      "2022-03-02 06:11:25,605 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.query.weight\n",
      "2022-03-02 06:11:25,606 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.value.bias\n",
      "2022-03-02 06:11:25,606 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.value.weight\n",
      "2022-03-02 06:11:25,608 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,608 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,608 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,608 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,609 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.dense.bias\n",
      "2022-03-02 06:11:25,609 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.dense.weight\n",
      "2022-03-02 06:11:25,609 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,610 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,610 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "2022-03-02 06:11:25,610 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "2022-03-02 06:11:25,611 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.key.bias\n",
      "2022-03-02 06:11:25,611 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.key.weight\n",
      "2022-03-02 06:11:25,611 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.query.bias\n",
      "2022-03-02 06:11:25,611 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.query.weight\n",
      "2022-03-02 06:11:25,612 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.value.bias\n",
      "2022-03-02 06:11:25,612 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.value.weight\n",
      "2022-03-02 06:11:25,612 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "2022-03-02 06:11:25,613 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "2022-03-02 06:11:25,613 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2022-03-02 06:11:25,614 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2022-03-02 06:11:25,614 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.dense.bias\n",
      "2022-03-02 06:11:25,614 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.dense.weight\n",
      "2022-03-02 06:11:25,614 - INFO - allennlp.nn.initializers -    bert_model.pooler.dense.bias\n",
      "2022-03-02 06:11:25,615 - INFO - allennlp.nn.initializers -    bert_model.pooler.dense.weight\n",
      "2022-03-02 06:11:25,615 - INFO - allennlp.nn.initializers -    tag_projection_layer.bias\n",
      "2022-03-02 06:11:25,615 - INFO - allennlp.nn.initializers -    tag_projection_layer.weight\n",
      "2022-03-02 06:11:25,828 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp9uola044\n",
      "/home/coraline/anaconda3/envs/bert-attack/lib/python3.7/site-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  'with `validate_args=False` to turn off validation.')\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.blank('en')\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "nlp.add_pipe(\"srl\")\n",
    "\n",
    "doc = nlp(\"The dog trashed the apartment in under 30 seconds. I am devastated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbs': [{'verb': 'trashed', 'description': '[ARG0: The dog] [V: trashed] [ARG1: the apartment] [ARGM-TMP: in under 30 seconds] . I am devastated .', 'tags': ['B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'am', 'description': 'The dog trashed the apartment in under 30 seconds . [ARG1: I] [V: am] [ARG2: devastated] .', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'B-V', 'B-ARG2', 'O']}, {'verb': 'devastated', 'description': 'The dog trashed the apartment in under 30 seconds . [ARG1: I] am [V: devastated] .', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'B-V', 'O']}], 'words': ['The', 'dog', 'trashed', 'the', 'apartment', 'in', 'under', '30', 'seconds', '.', 'I', 'am', 'devastated', '.']}\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree, ParentedTree\n",
    "\n",
    "#sent = list(doc.sents)[0]\n",
    "#parse_tree = ParentedTree.fromstring('(' + doc._.parse_string + ')')\n",
    "print(doc._.srl)\n",
    "#print(parse_tree.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-attack)",
   "language": "python",
   "name": "bert-attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
