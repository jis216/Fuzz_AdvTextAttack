{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import numpy\n",
    "\n",
    "from common.data_utils import get_dataset\n",
    "from model.tokenizer import PhraseTokenizer\n",
    "from model.attacker import Attacker\n",
    "from model.substitution import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1043cb044bdb495e81183822ee48103e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5ab6387b90437793a5184c98f9927c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce9a9f706664433aa3631d851b507cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1345000548.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples for the Target Sequence (multi-granuality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_seq = \"What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\"\n",
    "entry = {'text': tgt_seq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']\n"
     ]
    }
   ],
   "source": [
    "phrase_tok = PhraseTokenizer()\n",
    "phrase_token_output = phrase_tok.tokenize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what a pity',\n",
       " '!',\n",
       " 'frozen 2',\n",
       " 'is',\n",
       " 'bad',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'its predecessor',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'possibly',\n",
       " 'due',\n",
       " 'to',\n",
       " 'its chaotic production process',\n",
       " '.']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_token_output['phrases']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map phrase index to word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = 0\n",
    "p_s = 0\n",
    "p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "p_len = 0\n",
    "phrase2word = []\n",
    "new_p = True\n",
    "word_count = 0\n",
    "for w_s, w_e in phrase_token_output['word_offsets']:\n",
    "    \n",
    "    if new_p:\n",
    "        p_s = word_count\n",
    "        new_p = False\n",
    "    \n",
    "    if w_e == p_e:\n",
    "        phrase2word.append([p_s, word_count+1])\n",
    "        new_p = True\n",
    "        p_i = min(p_i + 1, len(phrase_token_output['phrase_offsets']) - 1)\n",
    "        p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "    \n",
    "    word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3],\n",
       " [3, 4],\n",
       " [4, 6],\n",
       " [6, 7],\n",
       " [7, 8],\n",
       " [8, 9],\n",
       " [9, 10],\n",
       " [10, 12],\n",
       " [12, 13],\n",
       " [13, 14],\n",
       " [14, 15],\n",
       " [15, 16],\n",
       " [16, 17],\n",
       " [17, 18],\n",
       " [18, 22],\n",
       " [22, 23]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 1 to phrase_len `[MASK]`' to the target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = []\n",
    "word2char = phrase_token_output['word_offsets']\n",
    "\n",
    "mask_index_list = []\n",
    "mask_count = 0\n",
    "for p_s, p_e in phrase2word:\n",
    "    if p_e - p_s >= 2:\n",
    "        c_s = word2char[ p_s ][0]\n",
    "        c_e = word2char[ p_e - 1][1]\n",
    "        \n",
    "        mask_len = p_e - p_s\n",
    "        for l in range(1, mask_len+1):\n",
    "            phrase_masked_list.append(tgt_seq[0:c_s] + ' [MASK]' * l + ' ' + tgt_seq[c_e:])\n",
    "            mask_index_list.append([mask_count, mask_count + l])\n",
    "            mask_count += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " ' [MASK] [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " ' [MASK] [MASK] [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity!  [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity!  [MASK] [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to  [MASK] , which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to  [MASK] [MASK] , which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] .',\n",
       " 'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] .',\n",
       " 'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] [MASK] .',\n",
       " 'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] [MASK] [MASK] .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get masked token candidates from MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 25])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 25, 30522])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_logits = torch.empty(len(mask_token_index), token_logits.shape[2])\n",
    "\n",
    "for i,ind in enumerate(mask_index_list):\n",
    "    li_s = mask_index_list[i][0]\n",
    "    li_e = mask_index_list[i][1]\n",
    "    ind_s = mask_token_index[li_s]\n",
    "    ind_e = mask_token_index[li_e - 1] + 1\n",
    "        \n",
    "    mask_token_logits[li_s:li_e] = token_logits[i, ind_s:ind_e, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=1).indices\n",
    "top_8_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here get_substitutes check the combination of word candidates and rank them by perplexity (cross_entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substitutes(substitutes, tokenizer, mlm_model):\n",
    "    # all substitutes  list of list of token-id (all candidates)\n",
    "    c_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    word_list = []\n",
    "\n",
    "    # find all possible candidates \n",
    "    all_substitutes = []\n",
    "    for i in range(substitutes.size(0)):\n",
    "        if len(all_substitutes) == 0:\n",
    "            lev_i = substitutes[i]\n",
    "            all_substitutes = [[int(c)] for c in lev_i]\n",
    "        else:\n",
    "            lev_i = []\n",
    "            for all_sub in all_substitutes:\n",
    "                for j in substitutes[i]:\n",
    "                    lev_i.append(all_sub + [int(j)])\n",
    "            all_substitutes = lev_i\n",
    "\n",
    "    # all_substitutes = all_substitutes[:24]\n",
    "    all_substitutes = torch.tensor(all_substitutes) # [ N, L ]\n",
    "    all_substitutes = all_substitutes[:24].to(device)\n",
    "    \n",
    "    print(all_substitutes.shape) # (K ^ t, K)\n",
    "\n",
    "    N, L = all_substitutes.size()\n",
    "    word_predictions = mlm_model(all_substitutes)[0] # N L vocab-size\n",
    "    ppl = c_loss(word_predictions.view(N*L, -1), all_substitutes.view(-1)) # [ N*L ] \n",
    "    ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-1)) # N  \n",
    "    \n",
    "    _, word_list = torch.sort(ppl)\n",
    "    word_list = [all_substitutes[i] for i in word_list]\n",
    "    final_words = []\n",
    "    for word in word_list[:24]:\n",
    "        tokens = [tokenizer._convert_id_to_token(int(i)) for i in word]\n",
    "        text = tokenizer.convert_tokens_to_string(tokens)\n",
    "        final_words.append(text)\n",
    "        \n",
    "    del all_substitutes\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n",
      "['go', 'ah', 'yo', 'yahoo', 'freeze', 'sorry', 'oh', 'sh']\n",
      " [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " go ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " ah ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " oh ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yo ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " sh ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([24, 2])\n",
      "['hey', 'never', 'yu', 'the', 'get', 'ice', 'o', 'ala']\n",
      "['up', 'go', 'attack', '##cha', 'it', 'you', 'out', '##bba']\n",
      " [MASK] [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yu it ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yu out ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " never up ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yubba ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yu go ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([24, 3])\n",
      "['the', 'sh', 'never', 'ya', 'yu', 'hey', 'aaa', 'o']\n",
      "['!', '-', '##hh', '##cha', ',', '##tar', \"'\", '##de']\n",
      "['##o', '##a', 'yo', 'you', '##m', '##h', '##g', '##s']\n",
      " [MASK] [MASK] [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " the !m ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " thehha ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " the -s ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " thehhs ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " the -a ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([8, 1])\n",
      "['it', 'this', '!', ',', 'which', 'that', 'production', 'sequel']\n",
      "What a pity!  [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  ,  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  it  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  that  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  this  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  which  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([24, 2])\n",
      "['the', 'this', 'its', 'that', 'second', ',', 'my', 'our']\n",
      "['film', 'sequel', 'movie', 'album', 'game', 'series', 'one', '2']\n",
      "What a pity!  [MASK] [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  the one  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  the film  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  the movie  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  this one  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  the series  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([8, 1])\n",
      "['frozen', 'it', 'predecessor', '3', 'original', 'first', 'its', '1']\n",
      "What a pity! Frozen 2 is bad compared to  [MASK] , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  it , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  its , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  3 , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  1 , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  first , which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([24, 2])\n",
      "['its', 'frozen', 'the', 'previous', 'ice', 'it', 'their', 'original']\n",
      "['predecessor', 'original', '1', 'predecessors', 'first', 'one', 'film', '2']\n",
      "What a pity! Frozen 2 is bad compared to  [MASK] [MASK] , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  the one , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  the film , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  the first , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  its film , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  the predecessor , which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([8, 1])\n",
      "['marketing', 'censorship', 'age', 'nostalgia', 'this', 'pacing', 'gameplay', 'hunger']\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  this .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  age .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  hunger .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  marketing .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  pacing .\n",
      "\n",
      "torch.Size([24, 2])\n",
      "['its', 'the', 'poor', 'some', 'technical', 'a', 'budget', 'several']\n",
      "['issues', 'gameplay', 'problems', 'differences', 'content', 'factors', 'difficulties', 'bugs']\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the difficulties .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the problems .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the bugs .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  its content .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the issues .\n",
      "\n",
      "torch.Size([24, 3])\n",
      "['its', 'the', 'a', 'poor', 'lack', 'an', 'some', 'increased']\n",
      "['of', 'poor', 'technical', 'and', 'gameplay', 'game', 'time', 'over']\n",
      "['engine', 'gameplay', 'issues', 'system', '##s', 'content', 'problems', 'difference']\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] [MASK] .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  its technical issues .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  its of engine .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  its of system .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  its of gameplay .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  its ofs .\n",
      "\n",
      "torch.Size([24, 4])\n",
      "['the', 'its', 'a', 'lack', 'an', 'inc', 'over', 'some']\n",
      "['lack', 'of', 'g', 'over', 'poor', 'and', \"'\", '-']\n",
      "['of', '##lit', 'and', 'in', 'game', 's', '##tri', '##per']\n",
      "['##s', 'gameplay', 'content', 'issues', 'engine', 'system', '##ing', 'problems']\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] [MASK] [MASK] .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lack of issues .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lack of engine .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lack of gameplay .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lacklit content .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lack of problems .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i, (p_s, p_e)) in enumerate(mask_index_list):\n",
    "    cur_phrase = ''\n",
    "    substitutes = top_8_tokens[p_s:p_e]\n",
    "    final_words = get_substitutes(substitutes, tokenizer, mlm_model)\n",
    "    for s in substitutes:\n",
    "        print(tokenizer.convert_ids_to_tokens(s))\n",
    "    \n",
    "    print(phrase_masked_list[i])\n",
    "    for w in final_words[:5]:\n",
    "        print(phrase_masked_list[i].replace((f' {tokenizer.mask_token}' * (p_e - p_s))[1:], w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = BertForSequenceClassification.from_pretrained('./data/imdb/saved_model/imdb_bert_base_uncased_finetuned_normal').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. retrieve logits and label from the target model\n",
    "inputs = tokenizer(entry['text'], return_tensors=\"pt\", truncation=True, max_length=512, return_token_type_ids=False)\n",
    "orig_logits = target_model(inputs['input_ids'].to(device), inputs['attention_mask'].to(device))[0].squeeze()\n",
    "orig_probs  = torch.softmax(orig_logits, -1)\n",
    "orig_label = torch.argmax(orig_probs)\n",
    "current_prob = orig_probs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask each phrase with `[UNK]` token and compute the confidence change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return units masked with UNK at each position in the sequence\n",
    "def _get_unk_masked(units):\n",
    "    len_text = len(units)\n",
    "    masked_units = []\n",
    "    for i in range(len_text - 1):\n",
    "        masked_units.append(units[0:i] + ['[UNK]'] + units[i + 1:])\n",
    "    \n",
    "    # list of masked basic units\n",
    "    return masked_units\n",
    "\n",
    "'''\n",
    "input units should be phrase tokens\n",
    "'''\n",
    "def get_important_scores(units, tgt_model, orig_prob, orig_label, orig_probs, tokenizer, batch_size=8, max_length=512):\n",
    "    masked_units = _get_unk_masked(units)\n",
    "    texts = [' '.join(units) for units in masked_units]  # list of text of masked units\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(tokenizer)\n",
    "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_token_type_ids=False, return_tensors='pt')\n",
    "    \n",
    "    eval_data = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n",
    "    leave_1_probs = []\n",
    "    \n",
    "    tgt_model.eval() #make sure in inference stage\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch[0].to(device)      # input ids\n",
    "            attention_mask = batch[1].to(device) # attention mask\n",
    "        \n",
    "            leave_1_prob_batch = tgt_model(input_ids, attention_mask=attention_mask)[0]\n",
    "            leave_1_probs.append(leave_1_prob_batch)\n",
    "        \n",
    "    leave_1_probs = torch.cat(leave_1_probs, dim=0)  # words, num-label\n",
    "    leave_1_probs = torch.softmax(leave_1_probs, -1)\n",
    "    leave_1_probs_argmax = torch.argmax(leave_1_probs, dim=-1)\n",
    "    import_scores = (orig_prob\n",
    "                     - leave_1_probs[:, orig_label] # how the probability of original label decreases\n",
    "                     +\n",
    "                     (leave_1_probs_argmax != orig_label).float() # new label not equal to original label\n",
    "                     * (leave_1_probs.max(dim=-1)[0] - torch.index_select(orig_probs, 0, leave_1_probs_argmax))\n",
    "                     ).data.cpu().numpy()           # probability of changed label\n",
    "\n",
    "    return import_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='bert-large-uncased-whole-word-masking', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "importance = get_important_scores(entry['phrases'], target_model, current_prob, orig_label, orig_probs, tokenizer, batch_size=8, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what a pity', 0.027897894),\n",
       " ('!', 0.010043323),\n",
       " ('bad', 0.007974029),\n",
       " ('frozen 2', 0.00469023),\n",
       " ('is', 0.0031422377),\n",
       " ('possibly', 0.003043294),\n",
       " ('its chaotic production process', 0.0026093125),\n",
       " ('its predecessor', 0.001532495),\n",
       " ('due', 0.0013412237),\n",
       " ('compared', 0.0013231635),\n",
       " ('to', 0.001247406),\n",
       " ('to', 0.00043827295),\n",
       " ('is', 0.00033521652),\n",
       " (',', -0.0005329251),\n",
       " ('which', -0.0006894469)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices = torch.argsort(torch.tensor(importance), dim=-1, descending=True)\n",
    "sorted_units = np.array(units)[sorted_indices]\n",
    "[(u,i) for (u,i) in zip(sorted_units, importance[sorted_indices])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask the Word 'Bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq.find('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a pity! Frozen 2 is  [MASK]  compared to its predecessor, which is possibly due to its chaotic production process.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list = (tgt_seq[0:25] + ' [MASK] ' + tgt_seq[28:])\n",
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a pity! Frozen 2 is  [MASK]  compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is disappointing compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is smaller compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is poorly compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is small compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is poor compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is short compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is weak compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is poorer compared to its predecessor, which is possibly due to its chaotic production process.\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for t in tokenizer.convert_ids_to_tokens(top_8_tokens[0]):\n",
    "    print(phrase_masked_list.replace(f' {tokenizer.mask_token} ', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Mask - implicit semantic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = tgt_seq\n",
    "\n",
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = tokenizer.convert_ids_to_tokens(inputs[0]).index('bad')\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is poor compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is worse compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is poorly compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is good compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is worst compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is disappointing compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is terrible compared to its predecessor, which is possibly due to its chaotic production process.\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for t in tokenizer.convert_ids_to_tokens(top_8_tokens):\n",
    "    print(phrase_masked_list.replace(f'bad', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "**Not deleting the word to be masked out does enforce semantic meaning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about multi-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = torch.tensor([1,2,3])\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 3])\n"
     ]
    }
   ],
   "source": [
    "final_words = get_substitutes(top_8_tokens, tokenizer, mlm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is drag',\n",
       " 'what is fate',\n",
       " 'what is shame',\n",
       " 'what an laugh',\n",
       " 'what a laugh',\n",
       " 'what a fate',\n",
       " 'what an fate',\n",
       " 'what an drag',\n",
       " 'what a pity',\n",
       " 'what is horror',\n",
       " 'what an shame',\n",
       " 'what an pity',\n",
       " 'what a horror',\n",
       " 'what an tragedy',\n",
       " 'what is laugh',\n",
       " 'what a drag',\n",
       " 'what a tragedy',\n",
       " 'what a depression',\n",
       " 'what a shame',\n",
       " 'what an horror',\n",
       " 'what an depression',\n",
       " 'what is tragedy',\n",
       " 'what is depression',\n",
       " 'what is pity']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_words 'what a pity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "what is drag! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "what is fate! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "what is shame! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "what an laugh! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "what a laugh! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for w in final_words[:5]:\n",
    "    print(phrase_masked_list.replace((f'What a pity'), w))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "**Should not do this for phrases. Since it still enforces single-word semangtic meaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE - universal sentence encoding (to finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c716b77a9bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/universal-sentence-encoder/4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m   \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 633\u001b[0;31m                             ckpt_options)\n\u001b[0m\u001b[1;32m    634\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         raise FileNotFoundError(\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m                                   self._checkpoint_options).expect_partial()\n\u001b[1;32m    329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         options=options)\n\u001b[1;32m   1319\u001b[0m     base.CheckpointPosition(\n\u001b[0;32m-> 1320\u001b[0;31m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[1;32m   1321\u001b[0m     load_status = CheckpointLoadStatus(\n\u001b[1;32m   1322\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    912\u001b[0m     restore_ops.extend(\n\u001b[1;32m    913\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[0;32m--> 914\u001b[0;31m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[1;32m    295\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[1;32m    296\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[0;32m--> 297\u001b[0;31m           validated_saveables).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[1;32m    298\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m           \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       restored_tensors = io_ops.restore_v2(\n\u001b[0;32m--> 104\u001b[0;31m           file_prefix, tensor_names, tensor_slices, tensor_dtypes)\n\u001b[0m\u001b[1;32m    105\u001b[0m     structured_restored_tensors = nest.pack_sequence_as(\n\u001b[1;32m    106\u001b[0m         tensor_structure, restored_tensors)\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mrestore_v2\u001b[0;34m(prefix, tensor_names, shape_and_slices, dtypes, name)\u001b[0m\n\u001b[1;32m   1510\u001b[0m       return restore_v2_eager_fallback(\n\u001b[1;32m   1511\u001b[0m           \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_and_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m           ctx=_ctx)\n\u001b[0m\u001b[1;32m   1513\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mrestore_v2_eager_fallback\u001b[0;34m(prefix, tensor_names, shape_and_slices, dtypes, name, ctx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m   _result = _execute.execute(b\"RestoreV2\", len(dtypes), inputs=_inputs_flat,\n\u001b[0;32m-> 1550\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1551\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/anaconda3/envs/bert-attack/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"./data/use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7498926"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embed([\n",
    "    \"what a pity\",\n",
    "    \"how unfortunate\"])\n",
    "\n",
    "np.dot(embeddings[0], embeddings[1]) #these are normalized embeddings, thus just need to compute product to get cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed([\n",
    "    \"what a pity\",\n",
    "    \"how unfortunate\"])\n",
    "\n",
    "np.dot(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       "array([[ 0.01118592, -0.02081541, -0.03585061, ..., -0.04640299,\n",
       "         0.00195996, -0.01107213],\n",
       "       [ 0.01757137, -0.02223784,  0.01493643, ..., -0.06386155,\n",
       "         0.03035415,  0.00380472]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5994162"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embed([\n",
    "    \"as well as\",\n",
    "    \"same as\"])\n",
    "\n",
    "np.dot(embeddings[0], embeddings[1]) #these are normalized embeddings, thus just need to compute product to get cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.590237"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embed([\n",
    "    \"love\",\n",
    "    \"hate\"])\n",
    "\n",
    "np.dot(embeddings[0], embeddings[1]) #these are normalized embeddings, thus just need to compute product to get cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16828361"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embed([\n",
    "    \"love\",\n",
    "    \"like\"])\n",
    "\n",
    "np.dot(embeddings[0], embeddings[1]) #these are normalized embeddings, thus just need to compute product to get cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[counter fitted embedding code](https://github.com/jind11/TextFooler/blob/master/comp_cos_sim_mat.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = './data/sim_mat/counter-fitted-vectors.txt'\n",
    "\n",
    "embeddings = []\n",
    "with open(embedding_path, 'r') as ifile:\n",
    "    for line in ifile:\n",
    "        embedding = [float(num) for num in line.strip().split()[1:]]\n",
    "        embeddings.append(embedding)\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.T.shape)\n",
    "norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings = np.asarray(embeddings / norm, \"float32\")\n",
    "product = np.dot(embeddings, embeddings.T)\n",
    "np.save(('cos_sim_counter_fitting.npy'), product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = './data/sim_mat/counter-fitted-vectors.txt'\n",
    "\n",
    "word_ids = {}\n",
    "embeddings = []\n",
    "    \n",
    "with open(embedding_path, 'r') as ifile:    \n",
    "    for i, line in enumerate(ifile):\n",
    "        line_tokens = line.strip().split()\n",
    "        word_ids[line_tokens[0]] = i + 1\n",
    "        embedding = np.array(line_tokens[1:], dtype='float64')\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        embeddings.append(embedding / norm)\n",
    "        \n",
    "embeddings = np.array(embeddings)\n",
    "#cos_sim_mat = np.dot(embeddings, embeddings.T)\n",
    "\n",
    "#np.save('./data/sim_mat/cos_sim_mat.npy', cos_sim_mat)\n",
    "#np.save('./data/sim_mat/word_vocab_id.npy', word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65713, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65713, 65713)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_mat_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.0119318 ,  0.00573509],\n",
       "       [-0.0119318 ,  1.        ,  0.05574999],\n",
       "       [ 0.00573509,  0.05574999,  1.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.0119318 ,  0.00573509],\n",
       "       [-0.0119318 ,  1.        ,  0.05574999],\n",
       "       [ 0.00573509,  0.05574999,  1.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_mat[0] / np.linalg.norm(embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "a = embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = embeddings[1] / np.linalg.norm(embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(a[1] == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_mat_loaded = np.load('./data/sim_mat/cos_sim_mat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_loaded = np.load('./data/sim_mat/embeddings_cf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids_loaded = np.load('./data/sim_mat/word_id.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 1, 'schlegel': 2, 'nunnery': 3}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.0119318 ,  0.00573509],\n",
       "       [-0.0119318 ,  1.        ,  0.05574999],\n",
       "       [ 0.00573509,  0.05574999,  1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_mat_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "read_dictionary = np.load('my_file.npy',allow_pickle='TRUE').item()\n",
    "print(read_dictionary['hello']) # displays \"world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "mixed_precision = False\n",
    "try:\n",
    "  from apex import amp\n",
    "except ImportError:\n",
    "  mixed_precision = False\n",
    "  \n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "  BertTokenizerFast,\n",
    "  AutoModelForMaskedLM,\n",
    "  BertForSequenceClassification,\n",
    ")\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "from common.data_utils import get_dataset\n",
    "from model.tokenizer import PhraseTokenizer\n",
    "from model.attacker import Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/coraline/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset(split_rate=0.8)\n",
    "train_ds = datasets.Dataset.from_dict(train_ds[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:resolver HttpCompressedFileResolver does not support the provided handle.\n",
      "INFO:absl:resolver GcsCompressedFileResolver does not support the provided handle.\n",
      "INFO:absl:resolver HttpUncompressedFileResolver does not support the provided handle.\n"
     ]
    }
   ],
   "source": [
    "encoder_use = hub.load(\"./data/use\") #url: https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "  \n",
    "embeddings_cf = np.load('./data/sim_mat/embeddings_cf.npy')\n",
    "word_ids = np.load('./data/sim_mat/word_id.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "phrase_tokenizer = PhraseTokenizer()\n",
    "target_model = BertForSequenceClassification.from_pretrained('./data/imdb/saved_model/imdb_bert_base_uncased_finetuned_normal').to(device)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.eval()\n",
    "mlm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a092aef8edc49e982e5b23e6b3e2b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.map(phrase_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    entry = train_ds[0]\n",
    "    \n",
    "    encoded = tokenizer(entry['text'],\n",
    "                             padding=True,\n",
    "                             truncation=True,\n",
    "                             return_token_type_ids=False,\n",
    "                             return_tensors=\"pt\")\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    orig_logits = target_model(input_ids, attention_mask).logits.squeeze()\n",
    "    orig_probs  = torch.softmax(orig_logits, -1)\n",
    "    orig_label = torch.argmax(orig_probs)\n",
    "    max_prob = torch.max(orig_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tokenizer import filter_unwanted_phrases, phrase_is_wanted\n",
    "from model.substitution import (\n",
    "  get_important_scores,\n",
    "  get_substitutes,\n",
    "  get_unk_masked,\n",
    "  get_phrase_masked_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = phrase_tokenizer.spacy_tokenizer.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indices = filter_unwanted_phrases(stop_words, entry['phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    masked_phrases = get_unk_masked(entry['text'], entry['phrase_offsets'], filtered_indices)\n",
    "    importance_scores, _ = get_important_scores(masked_phrases,\n",
    "                                                tokenizer,\n",
    "                                                target_model,\n",
    "                                                orig_label,\n",
    "                                                max_prob,\n",
    "                                                orig_probs,\n",
    "                                                device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the index after the filter and\n",
    "# cannot only applied to importance scores and filtered_indices\n",
    "sorted_filtered_indices_np = torch.argsort(importance_scores, dim=-1, descending=True).data.cpu().numpy()\n",
    "importance_scores_np = importance_scores.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain correct indices that can be used to index the entry dict\n",
    "sorted_indices_np = np.array(filtered_indices)[sorted_filtered_indices_np]\n",
    "sorted_importance = importance_scores_np[sorted_filtered_indices_np]\n",
    "sorted_phrases = np.array(entry['phrases'])[sorted_indices_np]\n",
    "sorted_phrase_offsets = np.array(entry['phrase_offsets'])[sorted_indices_np]\n",
    "sorted_n_words_in_phrase = np.array(entry['n_words_in_phrases'])[sorted_indices_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_change_threshold = len(filtered_indices)\n",
    "#  print(max_change_threshold)\n",
    "entry['success'] = False\n",
    "# record how many perturbations have been made\n",
    "changes = 0\n",
    "text = entry['text']\n",
    "phrases = entry['phrases']\n",
    "phrase_offsets = entry['phrase_offsets']\n",
    "n_words_in_phrases = entry['n_words_in_phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 10,  2, 13, 43, 38,  6, 41,  5, 36, 16, 25, 11, 54, 29,  1,  0,\n",
       "       46, 34, 14, 51, 15, 52, 53, 27, 22, 30, 37, 39, 28, 40, 35, 19, 55,\n",
       "        7,  3, 50, 12, 42, 24, 21, 26, 45,  4, 48, 17, 49, 44, 18, 32,  9,\n",
       "       31, 23, 20,  8, 33])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_filtered_indices_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 43\n",
    "n_words_in_phrases[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = get_phrase_masked_list(text,\n",
    "                                            [phrase_offsets[i]],\n",
    "                                            [n_words_in_phrases[i]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = len(phrase_masked_list) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text = phrase_masked_list[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for j, masked_text in enumerate(phrase_masked_list):\n",
    "    # 3. get masked token candidates from MLM\n",
    "encoded = tokenizer(masked_text,\n",
    "                         truncation=True,\n",
    "                         padding=True,\n",
    "                         return_token_type_ids=False,\n",
    "                         return_tensors='pt')\n",
    "input_ids = encoded['input_ids'].to(device)\n",
    "attention_mask = encoded['attention_mask'].to(device)\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = phrase_tokenizer.spacy_tokenizer.Defaults.stop_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the insightful students'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list = []\n",
    "if len(mask_token_index) == 1:\n",
    "    candidates_list = get_word_substitues(input_ids, attention_mask, tokenizer, mlm_model, K=8, threshold=3.0)\n",
    "elif len(mask_token_index) > 1:\n",
    "    candidates_list = get_phrase_substitutes(input_ids, attention_mask, mask_token_index, stop_words, tokenizer, mlm_model, device, beam_width=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['surviving', 'young', 'men'],\n",
       " ['surviving', 'young', 'girls'],\n",
       " ['surviving', 'young', 'boys'],\n",
       " ['living', 'young', 'ladies'],\n",
       " ['living', 'young', 'adults'],\n",
       " ['living', 'young', 'girls'],\n",
       " ['finding', 'parents', 'alive'],\n",
       " ['living', 'young', 'women'],\n",
       " ['knowing', 'people', 'inside'],\n",
       " ['surviving', 'young', 'students']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: set semantic threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18997534\n",
      "0.17179625\n",
      "0.17298445\n",
      "0.23151733\n",
      "0.2267068\n",
      "0.24183308\n",
      "0.16333567\n",
      "0.23965755\n",
      "0.21732841\n",
      "0.46957785\n"
     ]
    }
   ],
   "source": [
    "semantic_thres = 0.3\n",
    "K = 8\n",
    "attack_results = []\n",
    "\n",
    "mask_text = f\" {' '.join([tokenizer.mask_token] * (j+1))} \"\n",
    "\n",
    "for candidates in candidates_list:\n",
    "  perturbed_text = masked_text\n",
    "  candidate = ' '.join(candidates)\n",
    "    \n",
    "  if phrases[i] == candidate:\n",
    "    continue\n",
    "\n",
    "  if '##' in candidate:\n",
    "    continue\n",
    "\n",
    "  if not phrase_is_wanted(stop_words, candidate):\n",
    "    continue\n",
    "    \n",
    "  # semantic check\n",
    "  if len(candidates) > 1:\n",
    "    use_embeddings = encoder_use([candidate, phrases[i]])\n",
    "    phrase_sim =  np.dot(*use_embeddings)\n",
    "    print(phrase_sim)\n",
    "    if phrase_sim < semantic_thres:\n",
    "      continue\n",
    "  \n",
    "  \n",
    "  perturbed_text = perturbed_text.replace(mask_text, candidate, 1)\n",
    "  #  print(perturbed_text)\n",
    "\n",
    "  importance_score, perturbed_label = get_important_scores([perturbed_text],\n",
    "                                                           tokenizer,\n",
    "                                                           target_model,\n",
    "                                                           orig_label,\n",
    "                                                           max_prob,\n",
    "                                                           orig_probs,\n",
    "                                                           device)\n",
    "  importance_score = importance_score.squeeze()\n",
    "  perturbed_label = perturbed_label.squeeze()\n",
    "  #  print(orig_label == perturbed_label)\n",
    "  #  print(importance_score)\n",
    "  attack_results.append((perturbed_label == orig_label, j, candidate, perturbed_text, importance_score.item()))\n",
    "\n",
    "  if len(attack_results) == K:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97793984"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_embeddings = encoder_use(['This movie is absolutely horrible. The actor serves me the worst performance I have ever seen.', 'This movie is absolutely horrendous. The actor serves me the worst performance I have ever seen.'])\n",
    "np.dot(*use_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(True, device='cuda:0'),\n",
       "  2,\n",
       "  'surviving young students',\n",
       "  'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, surviving young students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!',\n",
       "  0.0178644061088562)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_results = sorted(attack_results, key=lambda x: x[-1], reverse=True)\n",
    "attack_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['popular', 'generally', 'online'],\n",
       " ['commercial', 'far', '##ce'],\n",
       " ['popular', 'economically', 'speaking'],\n",
       " ['popular', 'commercially', '##ised'],\n",
       " ['commercial', 'business', 'class'],\n",
       " ['popular', 'usually', 'live'],\n",
       " ['series', '##s', 'magazine'],\n",
       " ['popular', 'usually', 'speaking'],\n",
       " ['commercial', 'company', 'productions'],\n",
       " ['commercial', 'business', 'school'],\n",
       " ['commercial', 'far', '##aday'],\n",
       " ['commercial', 'film', 'school'],\n",
       " ['popular', 'commercially', 'available'],\n",
       " ['commercial', 'relatively', 'minor'],\n",
       " ['commercial', 'business', 'training'],\n",
       " ['popular', 'commercially', 'run'],\n",
       " ['popular', 'historically', 'speaking'],\n",
       " ['popularity', 'especially', 'popular'],\n",
       " ['popular', 'commercially', 'online'],\n",
       " ['popular', 'generally', 'overnight']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a cartoon comedy'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is  [MASK] [MASK] [MASK] . It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get substitutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_substitues(input_ids, attention_mask, tokenizer, mlm_model, K=8, threshold=3.0):\n",
    "    masked_logits = mlm_model(input_ids, attention_mask).logits\n",
    "    masked_logits = torch.index_select(masked_logits, 1, mask_token_index)\n",
    "    #  print(masked_logits.shape)\n",
    "    top_k_ids = torch.topk(masked_logits, K, dim=-1).indices[0]\n",
    "    substitute_scores = masked_logits[0,0][top_k_ids][0]\n",
    "    substitute_ids = top_k_ids[0]\n",
    "    \n",
    "    words = []\n",
    "    for (i, score) in zip(substitute_ids, substitute_scores):\n",
    "        if threshold != 0 and score < threshold:\n",
    "            break\n",
    "        words.append([tokenizer._convert_id_to_token(int(i))])\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cf = np.load('./data/sim_mat/embeddings_cf.npy')\n",
    "word_ids = np.load('./data/sim_mat/word_id.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07650426"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embeddings_cf[word_ids['france']], embeddings_cf[word_ids['french']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparable = False\n",
    "e1 = None\n",
    "if original_phrase in embed_vocab\n",
    "    comparable = True\n",
    "    e1 = embeddings[embed_vocab[original_phrase]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer('excercised')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'ex', '##cer', '##cise', '##d', '[SEP]']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_substitutes(input_ids, attention_mask, mask_token_index, stop_words, tokenizer, mlm_model, device, beam_width=10):\n",
    "    # all substitutes  list of list of token-id (all candidates)\n",
    "    c_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    word_positions = len(mask_token_index)\n",
    "        \n",
    "    \n",
    "    masked_logits = mlm_model(input_ids, attention_mask).logits\n",
    "    masked_logits = torch.index_select(masked_logits, 1, mask_token_index[0])\n",
    "    \n",
    "    # top_ids has a beam_width number of word combinations with smallest perplexities\n",
    "    # the initial candidates are the beam_width number of words with the highest logits\n",
    "    top_ids = torch.topk(masked_logits, beam_width**2, dim=-1).indices[0, 0]\n",
    "\n",
    "    #need to be passed in\n",
    "    filtered_indices = filter_unwanted_phrases(stop_words, tokenizer.convert_ids_to_tokens(top_ids))\n",
    "    \n",
    "    #initialize candidates pool with the top k candidates at the first position\n",
    "    candidate_ids = top_ids[filtered_indices][:beam_width].unsqueeze(0).T \n",
    "    \n",
    "    for p in range(1, word_positions):\n",
    "        \n",
    "        # cur_options = (beam_width, beam_width)\n",
    "        cur_options = torch.empty((len(candidate_ids) * beam_width, p+1), dtype=torch.long).to(device)\n",
    "        for a in range(len(candidate_ids)):\n",
    "            input_ids[0, mask_token_index[:p]] = candidate_ids[a]\n",
    "\n",
    "            masked_logits = mlm_model(input_ids, attention_mask).logits\n",
    "            masked_logits = torch.index_select(masked_logits, 1, mask_token_index[p])\n",
    "\n",
    "            #top_ids = torch.topk(masked_logits, beam_width**2, dim=-1).indices[0,0]\n",
    "            # filter words\n",
    "            #filtered_indices = filter_unwanted_phrases(stop_words, tokenizer.convert_ids_to_tokens(top_ids))\n",
    "            #new_ids = top_ids[filtered_indices][:beam_width]\n",
    "            \n",
    "            _, sorted_ids = torch.sort(masked_logits[0,0], dim=-1, descending=True)\n",
    "            pattern = re.compile(\"[\\W\\d_]+\")\n",
    "            \n",
    "            count = 0\n",
    "            new_ids = []\n",
    "            #print(sorted_ids)\n",
    "            for i in sorted_ids:\n",
    "                word = tokenizer.convert_ids_to_tokens(torch.tensor([i]))[0]\n",
    "                if word not in stop_words and pattern.fullmatch(word) is None:\n",
    "                    new_ids.append(i)\n",
    "                    count += 1\n",
    "                if count == beam_width:\n",
    "                    break\n",
    "            #print(new_ids)\n",
    "            \n",
    "            for b in range(beam_width):\n",
    "                #print(candidate_ids[a])\n",
    "                #print(new_ids[b])\n",
    "                #print(cur_options.shape, a*beam_width + b)\n",
    "                cur_options[a*beam_width + b] = torch.cat((candidate_ids[a], torch.tensor([new_ids[b]]).to(device)))\n",
    "\n",
    "        N, L = cur_options.size()\n",
    "        logits = mlm_model(cur_options)[0]\n",
    "\n",
    "        ppl = c_loss(logits.view(N*L, -1), cur_options.view(-1))\n",
    "        ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-1))\n",
    "\n",
    "        # the smaller the perplexity, the more coherent the sequence is\n",
    "        sorted_indices = torch.argsort(ppl)\n",
    "        candidate_ids = torch.index_select(cur_options, 0, sorted_indices)[:beam_width]\n",
    "\n",
    "    \n",
    "    sorted_token_ids_list = candidate_ids.tolist()\n",
    "    tokens_list = [tokenizer.convert_ids_to_tokens(tokens) for tokens in sorted_token_ids_list]\n",
    "    pprint(tokens_list)\n",
    "    \n",
    "    # necessary step to remove subwords\n",
    "    candidates_list = [[tokenizer.convert_tokens_to_string([token]) for token in tokens] for tokens in tokens_list]\n",
    "    \n",
    "    \n",
    "    return candidates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-attack)",
   "language": "python",
   "name": "bert-attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
