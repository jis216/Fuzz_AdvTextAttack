{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from common.data_utils import get_dataset\n",
    "from model.tokenizer import PhraseTokenizer\n",
    "from model.attacker import Attacker\n",
    "from model.substitution import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3]])\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.repeat(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples for the Target Sequence (multi-granuality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tokenizer import PhraseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_seq = \"Frozen 2 is bad compared to its predecessor. You cannot look into the story too much.\"\n",
    "entry = {'text': tgt_seq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_phrases']\n"
     ]
    }
   ],
   "source": [
    "phrase_tok = PhraseTokenizer()\n",
    "phrase_token_output = phrase_tok.tokenize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frozen',\n",
       " '2',\n",
       " 'is',\n",
       " 'bad',\n",
       " 'compared to',\n",
       " 'its',\n",
       " 'predecessor',\n",
       " '.',\n",
       " 'you',\n",
       " 'cannot',\n",
       " 'look into',\n",
       " 'the',\n",
       " 'story',\n",
       " 'too',\n",
       " 'much',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_token_output['phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6),\n",
       " (7, 8),\n",
       " (9, 11),\n",
       " (12, 15),\n",
       " (16, 27),\n",
       " (28, 31),\n",
       " (32, 43),\n",
       " (43, 44),\n",
       " (45, 48),\n",
       " (49, 55),\n",
       " (56, 65),\n",
       " (66, 69),\n",
       " (70, 75),\n",
       " (76, 79),\n",
       " (80, 84),\n",
       " (84, 85)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_token_output['phrase_offsets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map phrase index to word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = 0\n",
    "p_s = 0\n",
    "p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "p_len = 0\n",
    "phrase2word = []\n",
    "new_p = True\n",
    "word_count = 0\n",
    "for w_s, w_e in phrase_token_output['word_offsets']:\n",
    "    \n",
    "    if new_p:\n",
    "        p_s = word_count\n",
    "        new_p = False\n",
    "    \n",
    "    if w_e == p_e:\n",
    "        phrase2word.append([p_s, word_count+1])\n",
    "        new_p = True\n",
    "        p_i = min(p_i + 1, len(phrase_token_output['phrase_offsets']) - 1)\n",
    "        p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "    \n",
    "    word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [1, 2],\n",
       " [2, 3],\n",
       " [3, 4],\n",
       " [4, 6],\n",
       " [6, 7],\n",
       " [7, 8],\n",
       " [8, 9],\n",
       " [9, 10],\n",
       " [10, 11],\n",
       " [11, 13],\n",
       " [13, 14],\n",
       " [14, 15],\n",
       " [15, 16],\n",
       " [16, 17],\n",
       " [17, 18]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 1 to phrase_len `[MASK]`' to the target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = []\n",
    "word2char = phrase_token_output['word_offsets']\n",
    "\n",
    "mask_index_list = []\n",
    "mask_count = 0\n",
    "for p_s, p_e in phrase2word:\n",
    "    if p_e - p_s >= 2:\n",
    "        c_s = word2char[ p_s ][0]\n",
    "        c_e = word2char[ p_e - 1][1]\n",
    "        \n",
    "        mask_len = p_e - p_s\n",
    "        for l in range(1, mask_len+1):\n",
    "            phrase_masked_list.append(tgt_seq[0:c_s] + ' [MASK]' * l + ' ' + tgt_seq[c_e:])\n",
    "            mask_index_list.append([mask_count, mask_count + l])\n",
    "            mask_count += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frozen 2 is bad compared to its predecessor. You cannot look into the story too much.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frozen 2 is bad  [MASK]  its predecessor. You cannot look into the story too much.',\n",
       " 'Frozen 2 is bad  [MASK] [MASK]  its predecessor. You cannot look into the story too much.',\n",
       " 'Frozen 2 is bad compared to its predecessor. You cannot  [MASK]  the story too much.',\n",
       " 'Frozen 2 is bad compared to its predecessor. You cannot  [MASK] [MASK]  the story too much.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get masked token candidates from MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 30522])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_logits = torch.empty(len(mask_token_index), token_logits.shape[2])\n",
    "\n",
    "for i,ind in enumerate(mask_index_list):\n",
    "    li_s = mask_index_list[i][0]\n",
    "    li_e = mask_index_list[i][1]\n",
    "    ind_s = mask_token_index[li_s]\n",
    "    ind_e = mask_token_index[li_e - 1] + 1\n",
    "        \n",
    "    mask_token_logits[li_s:li_e] = token_logits[i, ind_s:ind_e, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=1).indices\n",
    "top_8_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here get_substitutes check the combination of word candidates and rank them by perplexity (cross_entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substitutes(substitutes, tokenizer, mlm_model):\n",
    "    # all substitutes  list of list of token-id (all candidates)\n",
    "    c_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    word_list = []\n",
    "\n",
    "    # find all possible candidates \n",
    "    all_substitutes = []\n",
    "    for i in range(substitutes.size(0)):\n",
    "        if len(all_substitutes) == 0:\n",
    "            lev_i = substitutes[i]\n",
    "            all_substitutes = [[int(c)] for c in lev_i]\n",
    "        else:\n",
    "            lev_i = []\n",
    "            for all_sub in all_substitutes:\n",
    "                for j in substitutes[i]:\n",
    "                    lev_i.append(all_sub + [int(j)])\n",
    "            all_substitutes = lev_i\n",
    "\n",
    "    # all_substitutes = all_substitutes[:24]\n",
    "    all_substitutes = torch.tensor(all_substitutes) # [ N, L ]\n",
    "    all_substitutes = all_substitutes[:24].to(device)\n",
    "    \n",
    "    print(all_substitutes.shape) # (K ^ t, K)\n",
    "\n",
    "    N, L = all_substitutes.size()\n",
    "    word_predictions = mlm_model(all_substitutes)[0] # N L vocab-size\n",
    "    ppl = c_loss(word_predictions.view(N*L, -1), all_substitutes.view(-1)) # [ N*L ] \n",
    "    ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-1)) # N  \n",
    "    \n",
    "    _, word_list = torch.sort(ppl)\n",
    "    word_list = [all_substitutes[i] for i in word_list]\n",
    "    final_words = []\n",
    "    for word in word_list[:24]:\n",
    "        tokens = [tokenizer._convert_id_to_token(int(i)) for i in word]\n",
    "        text = tokenizer.convert_tokens_to_string(tokens)\n",
    "        final_words.append(text)\n",
    "        \n",
    "    del all_substitutes\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n",
      "['as', 'like', 'for', 'after', 'enough', 'unlike', 'than', 'from']\n",
      "Frozen 2 is bad  [MASK]  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  as  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  for  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  after  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  like  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  than  its predecessor. You cannot look into the story too much.\n",
      "\n",
      "torch.Size([24, 2])\n",
      "['enough', 'compared', 'news', ',', 'as', 'business', 'tempered', 'looking']\n",
      "['as', 'like', 'to', 'from', 'than', 'for', 'of', 'with']\n",
      "Frozen 2 is bad  [MASK] [MASK]  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  news of  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  news for  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  news to  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  enough like  its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad  enough as  its predecessor. You cannot look into the story too much.\n",
      "\n",
      "torch.Size([8, 1])\n",
      "['enjoy', 'change', 'watch', 'spoil', 'read', 'like', 'delay', 'stress']\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  [MASK]  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  like  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  change  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  watch  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  read  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  enjoy  the story too much.\n",
      "\n",
      "torch.Size([24, 2])\n",
      "['focus', 're', 'take', 'keep', 'get', 'go', 'really', 'think']\n",
      "['about', 'on', 'into', 'to', 'in', 'with', 'enjoy', 'from']\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  [MASK] [MASK]  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  re to  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  take to  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  re on  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  focus to  the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot  re in  the story too much.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i, (p_s, p_e)) in enumerate(mask_index_list):\n",
    "    cur_phrase = ''\n",
    "    substitutes = top_8_tokens[p_s:p_e]\n",
    "    final_words = get_substitutes(substitutes, tokenizer, mlm_model)\n",
    "    for s in substitutes:\n",
    "        print(tokenizer.convert_ids_to_tokens(s))\n",
    "    \n",
    "    print(phrase_masked_list[i])\n",
    "    for w in final_words[:5]:\n",
    "        print(phrase_masked_list[i].replace((f' {tokenizer.mask_token}' * (p_e - p_s))[1:], w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "target_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/distilbert-base-uncased-imdb\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. retrieve logits and label from the target model\n",
    "inputs = tokenizer(entry['text'], return_tensors=\"pt\", truncation=True, max_length=512, return_token_type_ids=False)\n",
    "orig_logits = target_model(inputs['input_ids'].to(device), inputs['attention_mask'].to(device))[0].squeeze()\n",
    "orig_probs  = torch.softmax(orig_logits, -1)\n",
    "orig_label = torch.argmax(orig_probs)\n",
    "current_prob = orig_probs.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask each phrase with `[UNK]` token and compute the confidence change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return units masked with UNK at each position in the sequence\n",
    "def _get_unk_masked(units):\n",
    "    len_text = len(units)\n",
    "    masked_units = []\n",
    "    for i in range(len_text - 1):\n",
    "        masked_units.append(units[0:i] + ['[UNK]'] + units[i + 1:])\n",
    "    \n",
    "    # list of masked basic units\n",
    "    return masked_units\n",
    "\n",
    "'''\n",
    "input units should be phrase tokens\n",
    "'''\n",
    "def get_important_scores(units, tgt_model, orig_prob, orig_label, orig_probs, tokenizer, batch_size=8, max_length=512):\n",
    "    masked_units = _get_unk_masked(units)\n",
    "    texts = [' '.join(units) for units in masked_units]  # list of text of masked units\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(tokenizer)\n",
    "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_token_type_ids=False, return_tensors='pt')\n",
    "    \n",
    "    eval_data = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n",
    "    leave_1_probs = []\n",
    "    \n",
    "    tgt_model.eval() #make sure in inference stage\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch[0].to(device)      # input ids\n",
    "            attention_mask = batch[1].to(device) # attention mask\n",
    "        \n",
    "            leave_1_prob_batch = tgt_model(input_ids, attention_mask=attention_mask)[0]\n",
    "            leave_1_probs.append(leave_1_prob_batch)\n",
    "        \n",
    "    leave_1_probs = torch.cat(leave_1_probs, dim=0)  # words, num-label\n",
    "    leave_1_probs = torch.softmax(leave_1_probs, -1)\n",
    "    leave_1_probs_argmax = torch.argmax(leave_1_probs, dim=-1)\n",
    "    import_scores = (orig_prob\n",
    "                     - leave_1_probs[:, orig_label] # how the probability of original label decreases\n",
    "                     +\n",
    "                     (leave_1_probs_argmax != orig_label).float() # new label not equal to original label\n",
    "                     * (leave_1_probs.max(dim=-1)[0] - torch.index_select(orig_probs, 0, leave_1_probs_argmax))\n",
    "                     ).data.cpu().numpy()           # probability of changed label\n",
    "\n",
    "    return import_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='bert-large-uncased-whole-word-masking', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "importance = get_important_scores(entry['phrases'], target_model, current_prob, orig_label, orig_probs, tokenizer, batch_size=8, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 1.3867681),\n",
       " ('.', 0.036233902),\n",
       " ('story', 0.019454658),\n",
       " ('predecessor', 0.015556216),\n",
       " ('the', 0.01033473),\n",
       " ('much', 0.007848859),\n",
       " ('its', 0.007283628),\n",
       " ('you', 0.005296588),\n",
       " ('frozen', 0.0013504624),\n",
       " ('look into', 0.00082850456),\n",
       " ('2', 0.00012338161),\n",
       " ('cannot', -0.0028142333),\n",
       " ('too', -0.007349491),\n",
       " ('compared to', -0.012966096),\n",
       " ('is', -0.016696215)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices = torch.argsort(torch.tensor(importance), dim=-1, descending=True).cpu()\n",
    "sorted_units = np.array(entry['phrases'])[sorted_indices]\n",
    "[(u,i) for (u,i) in zip(sorted_units, importance[sorted_indices])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask the Word 'Bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq.find('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frozen 2 is  [MASK]  compared to its predecessor. You cannot look into the story too much.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list = (tgt_seq[0:12] + ' [MASK] ' + tgt_seq[15:])\n",
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen 2 is  [MASK]  compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is nothing compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is tame compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is small compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is simple compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is weak compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is boring compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is disappointing compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is short compared to its predecessor. You cannot look into the story too much.\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for t in tokenizer.convert_ids_to_tokens(top_8_tokens[0]):\n",
    "    print(phrase_masked_list.replace(f' {tokenizer.mask_token} ', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Mask - implicit semantic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = tgt_seq\n",
    "\n",
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = tokenizer.convert_ids_to_tokens(inputs[0]).index('bad')\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen 2 is bad compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is good compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is poor compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is worst compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is horrible compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is like compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is worse compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is terrible compared to its predecessor. You cannot look into the story too much.\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for t in tokenizer.convert_ids_to_tokens(top_8_tokens):\n",
    "    print(phrase_masked_list.replace(f'bad', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "**Not deleting the word to be masked out does enforce semantic meaning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about multi-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = torch.tensor([12,13])\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 2])\n"
     ]
    }
   ],
   "source": [
    "final_words = get_substitutes(top_8_tokens, tokenizer, mlm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen 2 is bad compared to its predecessor. You cannot look into the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot see in the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot see into the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot see after the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot see about the story too much.\n",
      "Frozen 2 is bad compared to its predecessor. You cannot see behind the story too much.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for w in final_words[:5]:\n",
    "    print(phrase_masked_list.replace((f'look into'), w))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "**Should not do this for phrases. Since it still enforces single-word semangtic meaning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-attack)",
   "language": "python",
   "name": "bert-attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
