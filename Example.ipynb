{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5mRbRsqDUKPa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /home/coraline/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "import benepar\n",
    "import nltk\n",
    "\n",
    "from common.data_utils import get_dataset\n",
    "from model.tokenizer import PhraseTokenizer\n",
    "from model.attacker import Attacker\n",
    "from model.substitution import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1646679553958,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "tC2aVFD7UKPe",
    "outputId": "6ec5e341-181d-4236-c6e4-50bcdd90afca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10346,
     "status": "ok",
     "timestamp": 1646679565131,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "O394LT3gUKPf",
    "outputId": "9e4af2c7-865c-4296-e788-4562aa83ddcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkghxRMTUKPf"
   },
   "source": [
    "## Generate Adversarial Examples for the Target Sequence (multi-granuality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1646679569424,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "bcRMjmmsUKPn"
   },
   "outputs": [],
   "source": [
    "from model.tokenizer import PhraseTokenizer\n",
    "from fuzzingbook.GrammarFuzzer import display_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcQFJ5DJEkSa"
   },
   "source": [
    "Display parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1646679574017,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "7-BXSNP77LjU"
   },
   "outputs": [],
   "source": [
    "def _parse_error(s, match, expecting):\n",
    "    \"\"\"\n",
    "    Display a friendly error message when parsing a tree string fails.\n",
    "    :param s: The string we're parsing.\n",
    "    :param match: regexp match of the problem token.\n",
    "    :param expecting: what we expected to see instead.\n",
    "    \"\"\"\n",
    "    # Construct a basic error message\n",
    "    if match == \"end-of-string\":\n",
    "        pos, token = len(s), \"end-of-string\"\n",
    "    else:\n",
    "        pos, token = match.start(), match.group()\n",
    "    msg = \"read(): expected %r but got %r\\n%sat index %d.\" % (\n",
    "        expecting,\n",
    "        token,\n",
    "        \" \" * 12,\n",
    "        pos,\n",
    "    )\n",
    "    # Add a display showing the error token itsels:\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    offset = pos\n",
    "    if len(s) > pos + 10:\n",
    "        s = s[: pos + 10] + \"...\"\n",
    "    if pos > 10:\n",
    "        s = \"...\" + s[pos - 10 :]\n",
    "        offset = 13\n",
    "    msg += '\\n{}\"{}\"\\n{}^'.format(\" \" * 16, s, \" \" * (17 + offset))\n",
    "    raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1646679577135,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "w08nUQgM7M7j"
   },
   "outputs": [],
   "source": [
    "def fromstring(\n",
    "        s,\n",
    "        brackets=\"()\",\n",
    "        read_node=None,\n",
    "        read_leaf=None,\n",
    "        node_pattern=None,\n",
    "        leaf_pattern=None,\n",
    "        remove_empty_top_bracketing=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read a bracketed tree string and return the resulting tree.\n",
    "        Trees are represented as nested brackettings, such as::\n",
    "\n",
    "          (S (NP (NNP John)) (VP (V runs)))\n",
    "\n",
    "        :type s: str\n",
    "        :param s: The string to read\n",
    "\n",
    "        :type brackets: str (length=2)\n",
    "        :param brackets: The bracket characters used to mark the\n",
    "            beginning and end of trees and subtrees.\n",
    "\n",
    "        :type read_node: function\n",
    "        :type read_leaf: function\n",
    "        :param read_node, read_leaf: If specified, these functions\n",
    "            are applied to the substrings of ``s`` corresponding to\n",
    "            nodes and leaves (respectively) to obtain the values for\n",
    "            those nodes and leaves.  They should have the following\n",
    "            signature:\n",
    "\n",
    "               read_node(str) -> value\n",
    "\n",
    "            For example, these functions could be used to process nodes\n",
    "            and leaves whose values should be some type other than\n",
    "            string (such as ``FeatStruct``).\n",
    "            Note that by default, node strings and leaf strings are\n",
    "            delimited by whitespace and brackets; to override this\n",
    "            default, use the ``node_pattern`` and ``leaf_pattern``\n",
    "            arguments.\n",
    "\n",
    "        :type node_pattern: str\n",
    "        :type leaf_pattern: str\n",
    "        :param node_pattern, leaf_pattern: Regular expression patterns\n",
    "            used to find node and leaf substrings in ``s``.  By\n",
    "            default, both nodes patterns are defined to match any\n",
    "            sequence of non-whitespace non-bracket characters.\n",
    "\n",
    "        :type remove_empty_top_bracketing: bool\n",
    "        :param remove_empty_top_bracketing: If the resulting tree has\n",
    "            an empty node label, and is length one, then return its\n",
    "            single child instead.  This is useful for treebank trees,\n",
    "            which sometimes contain an extra level of bracketing.\n",
    "\n",
    "        :return: A tree corresponding to the string representation ``s``.\n",
    "            If this class method is called using a subclass of Tree,\n",
    "            then it will return a tree of that type.\n",
    "        :rtype: Tree\n",
    "        \"\"\"\n",
    "        if not isinstance(brackets, str) or len(brackets) != 2:\n",
    "            raise TypeError(\"brackets must be a length-2 string\")\n",
    "        if re.search(r\"\\s\", brackets):\n",
    "            raise TypeError(\"whitespace brackets not allowed\")\n",
    "        # Construct a regexp that will tokenize the string.\n",
    "        open_b, close_b = brackets\n",
    "        open_pattern, close_pattern = (re.escape(open_b), re.escape(close_b))\n",
    "        if node_pattern is None:\n",
    "            node_pattern = fr\"[^\\s{open_pattern}{close_pattern}]+\"\n",
    "        if leaf_pattern is None:\n",
    "            leaf_pattern = fr\"[^\\s{open_pattern}{close_pattern}]+\"\n",
    "        token_re = re.compile(\n",
    "            r\"%s\\s*(%s)?|%s|(%s)\"\n",
    "            % (open_pattern, node_pattern, close_pattern, leaf_pattern)\n",
    "        )\n",
    "        # Walk through each token, updating a stack of trees.\n",
    "        stack = [(None, [])]  # list of (node, children) tuples\n",
    "        for match in token_re.finditer(s):\n",
    "            token = match.group()\n",
    "            # Beginning of a tree/subtree\n",
    "            if token[0] == open_b:\n",
    "                if len(stack) == 1 and len(stack[0][1]) > 0:\n",
    "                    _parse_error(s, match, \"end-of-string\")\n",
    "                label = token[1:].lstrip()\n",
    "                if read_node is not None:\n",
    "                    label = read_node(label)\n",
    "                stack.append((label, []))\n",
    "            # End of a tree/subtree\n",
    "            elif token == close_b:\n",
    "                if len(stack) == 1:\n",
    "                    if len(stack[0][1]) == 0:\n",
    "                        _parse_error(s, match, open_b)\n",
    "                    else:\n",
    "                        _parse_error(s, match, \"end-of-string\")\n",
    "                label, children = stack.pop()\n",
    "                if len(children) > 0 and type(children[0]) == str:\n",
    "                    children = [(children[0], [])]\n",
    "                stack[-1][1].append((label, children))\n",
    "            # Leaf node\n",
    "            else:\n",
    "                if len(stack) == 1:\n",
    "                    _parse_error(s, match, open_b)\n",
    "                if read_leaf is not None:\n",
    "                    token = read_leaf(token)\n",
    "                stack[-1][1].append(token)\n",
    "\n",
    "        # check that we got exactly one complete tree.\n",
    "        if len(stack) > 1:\n",
    "            _parse_error(s, \"end-of-string\", close_b)\n",
    "        elif len(stack[0][1]) == 0:\n",
    "            _parse_error(s, \"end-of-string\", open_b)\n",
    "        else:\n",
    "            assert stack[0][0] is None\n",
    "            assert len(stack[0][1]) == 1\n",
    "        tree = stack[0][1][0]\n",
    "\n",
    "        # If the tree has an extra level with node='', then get rid of\n",
    "        # it.  E.g.: \"((S (NP ...) (VP ...)))\"\n",
    "        if remove_empty_top_bracketing and tree._label == \"\" and len(tree) == 1:\n",
    "            tree = tree[0]\n",
    "        # return the tree.\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 4549,
     "status": "ok",
     "timestamp": 1646679583815,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "xNVBUC6j7yng"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "def tree_parse2(input_sentence):\n",
    "    doc = nlp(input_sentence)\n",
    "\n",
    "    whole_string = '('\n",
    "    for tree in doc.sents:\n",
    "        whole_string += tree._.parse_string\n",
    "\n",
    "    return whole_string + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1646679613544,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "GRTqCbZmFst4"
   },
   "outputs": [],
   "source": [
    "# other examples for tokenizer\n",
    "# sentence = \"I will take over from now on.\"\n",
    "# sentence = \"The movie is great as far as I think.\"\n",
    "# sentence = \"The actors fall in love at first sight.\"\n",
    "# sentence = \"I will meet you at the train station.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11318,
     "status": "ok",
     "timestamp": 1646681096046,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "fmAEnP0CUKPo",
    "outputId": "1828501c-f64d-4d8c-d82f-1f406f57056b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'benepar', 'merge_phrases']\n"
     ]
    }
   ],
   "source": [
    "phrase_tok = PhraseTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1646681028806,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "19pvfLNUUKPo",
    "outputId": "f8fc1f06-f366-48cc-d561-dac6126072ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'are able to participate'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq = \"I have to disagree with these other reviews.  Presbyterian is a joke.  Unfortunately our primary care physician is right around the corner, and affiliated with Presbyterian as well, and so any time they want to order up something that they can't handle in the office, guess where they send us?\\\\n\\\\nTo make matters worse, the place is always full (or overfull), always understaffed, and always incapable of handling anything.  The emergency department is horrid, but to make matters worse, it's better than being admitted.\\\\n\\\\nIn one of the worst cases of mismanagement ever, what should have been a simple examination - passed from our regular doctor - ended up being at least 12 hours in the emergency room while we waited for a regular room, then several days (yes days) in the hospital itself while they proceeded to run a whopping one test per day.  That's right - one test per day!\\\\n\\\\nThe test would usually be run in mid-morning, and the results would come back mid-afternoon, at which point they would try and find someone to interpret the test results, which could take until after dinner, and by the time that happened, it would be night time, and then another test would be ordered for the next day.  It was simply pathetic.\\\\n\\\\nWhatever you do, do not go to the emergency room at Presbyterian.\"\n",
    "tgt_seq = \"Monday night drink specials and an amazingly musical performance will be provided to you with a 5 dollar cover.. What more could you ask for?\\\\n\\\\n\\\\nThis building breeds history and I was proud to experience it my last night in Charlotte.\"\n",
    "tgt_seq = \"Sundays used to be all you can drink vodka for 10 dollar cover... After paying our cover they let us know that beginning this week vodka drinks are 2 dollars... Why would anyone come on Sunday if they're pouring 2 dollar glasses of cranberry juice?\"\n",
    "tgt_seq = \"Though the Freedom Dr. K-Mart looks like the setting of a post-apocalyptic film and the entire chain is past its prime, you can find some 'trinkets' for cheap on the inside.  \\\\n\\\\nThis K-Mart has some affordable home furninishing knock-offs (by cheap, I mean cheaper than Target/IKEA).  While it some of it is dated, K-Mart still carries the much-touted Martha Stewart brands.  I've found a 'runner' rug and bedspread that both look like something from department stores.  The same can be said for the furniture, so don't hate on my stylish house. \\\\n\\\\nThe clothing department is slightly dicier, but I have scavenged some faddish clothes (think Gap 3 years ago) among the Big Bubba t-shirts and overalls.  Their young men's section has good deals at the end of the season, like utility shirts or those really metrosexual boot-cut jeans for less than $15.  Yeah....really.\\\\n\\\\nThe rest of the store is pretty much filler.  If you actually go to this K-Mart ( or any K-Mart) looking for a decent selection of sporting goods/shoes/electronics, you will likely pay more than other big box retailers or be appalled by the selection.  If you're like me and approach K-Mart shopping like thrift store diving, you will be pleasantly surprised. Can't decide then if visiting K-Mart is frugal or frivolous, but who doesn't need another cheap belt?\"\n",
    "tgt_seq = \"Ditto the other ratings for UVerse customer service, at least once you get past the fabulous sales people. My install was scheduled for between 9-11 am on a Saturday, dude called at 2:30 to say he was the \\\\\\\"\\\"outside guy\\\\\\\"\\\" and was on his way. Was indignant when I asked about the earlier time frame (actually I laughed when I answered the phone and found out who he was....3.5 hrs late!), but I let him come and do at least the outside work. Called the number on my scheduled appt email at 3:30, got a woman who knew nothing about my issues, tried to transfer me to dispatch, transfer failed. She called me back, I asked for a supervisor and was on hold for 15 minutes. She called back again, tried to transfer me to somebody else, said there was a \\\\\\\"\\\"problem\\\\\\\"\\\" with my order, but she couldn't tell me what it was. After another 20 minutes on hold, I hung up. She called back once more, told me she would call me as soon as the department was available - never heard back. Unbelievable! Not thrilled with Time Warner or most of their contracted install/repair folks, but at least when they are going to be late or have to reschedule, they call! DON'T DO IT!\"\n",
    "tgt_seq = \"Red alert. Red Alert. \\\\n\\\\nIf you are looking for good mobile phone customer service, this is not the store. I'm changing my provider to Verizon or AT&T after the experience I had last night. Here's how it went down....\\\\n\\\\nI headed in around 6:30pm as my contract expired so I needed to upgrade my phone and sign on for two more years. When I entered, I noticed around eight employees out front and three in the back...so I thought to myself, this won't take long, there are a ton of people working. I signed in with the \\\\\\\"\\\"receptionist\\\\\\\"\\\" who told me the wait to buy a new phone would be 10 minutes. No biggie, I told her, I'll just go check out all the gadgets. \\\\n\\\\nForty minutes later I was still waiting - and showing as third on the list for service. Now this wouldn't be a big deal if I wasn't watching the employees talk to each other and head into the back for long periods of time while customers were waiting out front. I literally timed two people between customers - one waited over 15 minutes before helping someone else and the second employee wasn't helping anyone when I left and it had been 20 minutes since his last customer. \\\\n\\\\nNeedless to say, I left the store...but not before telling them to take me off the list so the poor people in line after me didn't have to wait even longer. I will not be back and have been researching other providers today.\"\n",
    "tgt_seq = \"We went on a Saturday morning and had to wait almost an hour to be seated.  My egg white omelette was runny, so I sent it back.  We are now an hour and twenty minutes into our dining experience and I still have no food that is edible.  When the omelette returned, they had done something to the eggs to make them ultra-fluffy, which sounds like it would be good, but it wasn't.  It was literally like eating a kitchen sponge.  My son and husband got pancakes and said that the ones we make at home are better.\\\\n\\\\nOh, I should add that we went to the one by the Target near uptown.\"\n",
    "tgt_seq = \"Recently joined the YMCA and this is my \\\\\\\"\\\"home\\\\\\\"\\\" ymca.  I LOVE the equipment and 'most' of the staff.  I have hit the gym (been a member about 30 days) at very varied times but have settled on 5 am for strength training (mon/wed/fri) and 3pm (ish) for cardio (Tues/Thurs/Sat).  At either time it isn't overwhelmingly crowded.\\\\n\\\\nI will say at 5 am (and I assume other times) there are plenty of meat heads in the weight room - grunting, slamming weights, flexing in the mirror.  I also  find it entertaining to see the fashion show that occurs in the weight room (I like to ppl watch between sets).   I have NEVER seen so many coordinated work clothes in my entire life.  Matching shows, pants/shorts/shirts and often times hats.  None of this bothers me, it is just entertaining.\\\\n\\\\nI have not met anyone very friendly there and I assume that is because it is the \\\\\\\"\\\"SoPark\\\\\\\"\\\" area - but since I don't go to the gym to socialize I guess it doesn't matter.\"\n",
    "tgt_seq = \"Laser quest is a fun experience for kids of all ages, but the adults that play at this establishment, honestly kind of creep me out. The best time to bring your children here is during the middle of the week, because there is always an abundance of birthday parties on the weekend, and it can take hours before you are able to participate.\"\n",
    "tgt_seq = \"are able to participate\"\n",
    "entry = {'text': tgt_seq}\n",
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1646679721460,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "XGrmAhrzUKPp",
    "outputId": "e8f14014-4e51-4b75-811c-23f8ec56887a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coraline/anaconda3/envs/bert-attack/lib/python3.7/site-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  'with `validate_args=False` to turn off validation.')\n"
     ]
    }
   ],
   "source": [
    "phrase_token_output = phrase_tok.tokenize(entry)\n",
    "for phrase in phrase_token_output['phrases']:\n",
    "    if len(phrase.split()) > 1:\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((VP (VBP are) (ADJP (JJ able) (S (VP (TO to) (VP (VB participate)))))))'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_parse2(tgt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1646681037881,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "-y-txOZG71W2",
    "outputId": "1d82ee4e-c3d6-4982-f354-a9a167c9ff36"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"166pt\" height=\"365pt\"\n",
       " viewBox=\"0.00 0.00 166.00 365.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 361)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-361 162,-361 162,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41.5\" y=\"-309.8\" font-family=\"Times,serif\" font-size=\"14.00\">VP</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.5,-314.62C41.5,-314.6 41.5,-314.58 41.5,-314.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"45.03,-324.52 41.5,-314.53 38.03,-324.54 45.03,-324.52\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"15.5\" y=\"-258.8\" font-family=\"Times,serif\" font-size=\"14.00\">VBP</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M37.88,-305.69C34.28,-298.88 28.6,-288.19 23.87,-279.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"26.86,-277.45 19.08,-270.26 20.68,-280.73 26.86,-277.45\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"67.5\" y=\"-258.8\" font-family=\"Times,serif\" font-size=\"14.00\">ADJP</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.12,-305.69C48.72,-298.88 54.4,-288.19 59.13,-279.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.32,-280.73 63.92,-270.26 56.14,-277.45 62.32,-280.73\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"15.5\" y=\"-207.8\" font-family=\"Times,serif\" font-size=\"14.00\">are</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M15.5,-254.69C15.5,-248.18 15.5,-238.1 15.5,-229.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"19,-229.26 15.5,-219.26 12,-229.26 19,-229.26\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"59.5\" y=\"-207.8\" font-family=\"Times,serif\" font-size=\"14.00\">JJ</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.39,-254.69C65.31,-248.1 63.64,-237.87 62.22,-229.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.67,-228.56 60.6,-219.26 58.76,-229.69 65.67,-228.56\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-207.8\" font-family=\"Times,serif\" font-size=\"14.00\">S</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.39,-254.69C75.32,-247.81 81.52,-236.96 86.65,-227.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.72,-229.67 91.64,-219.26 83.64,-226.2 89.72,-229.67\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\">able</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58.94,-203.69C58.41,-197.18 57.59,-187.1 56.88,-178.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"60.35,-177.94 56.05,-168.26 53.38,-178.51 60.35,-177.94\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\">VP</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M96.06,-203.69C96.59,-197.18 97.41,-187.1 98.12,-178.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.62,-178.51 98.95,-168.26 94.65,-177.94 101.62,-178.51\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"79.5\" y=\"-105.8\" font-family=\"Times,serif\" font-size=\"14.00\">TO</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M96.72,-152.69C93.97,-145.96 89.67,-135.42 86.05,-126.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.28,-125.19 82.26,-117.26 82.8,-127.84 89.28,-125.19\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-105.8\" font-family=\"Times,serif\" font-size=\"14.00\">VP</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>8&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.14,-152.69C104.72,-146.03 108.75,-135.65 112.16,-126.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.53,-127.84 115.88,-117.26 109,-125.31 115.53,-127.84\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"79.5\" y=\"-54.8\" font-family=\"Times,serif\" font-size=\"14.00\">to</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79.5,-101.69C79.5,-95.18 79.5,-85.1 79.5,-76.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83,-76.26 79.5,-66.26 76,-76.26 83,-76.26\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-54.8\" font-family=\"Times,serif\" font-size=\"14.00\">VB</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.5,-101.69C118.5,-95.18 118.5,-85.1 118.5,-76.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122,-76.26 118.5,-66.26 115,-76.26 122,-76.26\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-3.8\" font-family=\"Times,serif\" font-size=\"14.00\">participate</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.5,-50.69C118.5,-44.18 118.5,-34.1 118.5,-25.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122,-25.26 118.5,-15.26 115,-25.26 122,-25.26\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6d64870790>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_tree(fromstring(tree_parse2(tgt_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3Ki4_xUKPr"
   },
   "source": [
    "### Map phrase index to word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1646681137518,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "otZgGur1UKPr"
   },
   "outputs": [],
   "source": [
    "p_i = 0\n",
    "p_s = 0\n",
    "p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "p_len = 0\n",
    "phrase2word = []\n",
    "new_p = True\n",
    "word_count = 0\n",
    "for w_s, w_e in phrase_token_output['word_offsets']:\n",
    "    \n",
    "    if new_p:\n",
    "        p_s = word_count\n",
    "        new_p = False\n",
    "    \n",
    "    if w_e == p_e:\n",
    "        phrase2word.append([p_s, word_count+1])\n",
    "        new_p = True\n",
    "        p_i = min(p_i + 1, len(phrase_token_output['phrase_offsets']) - 1)\n",
    "        p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "    \n",
    "    word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1646681138155,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "TPTsucKCUKPr",
    "outputId": "3a05ec60-cdc5-4eb2-99fc-41c9637dbf77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [1, 2],\n",
       " [2, 4],\n",
       " [4, 5],\n",
       " [5, 6],\n",
       " [6, 7],\n",
       " [7, 8],\n",
       " [8, 9],\n",
       " [9, 10],\n",
       " [10, 11],\n",
       " [11, 12],\n",
       " [12, 13],\n",
       " [13, 14],\n",
       " [14, 15],\n",
       " [15, 16],\n",
       " [16, 17],\n",
       " [17, 18],\n",
       " [18, 19]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1646681140055,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "9XDq-mpArtHI",
    "outputId": "6f736c36-03e0-4581-ec74-3b6ac809a92a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disney has \u001b[92mput out\u001b[0m too many bland live remakes now and beauty and the beast is no exception . \n"
     ]
    }
   ],
   "source": [
    "# set color for input word\n",
    "def color_words(word, color = 1):\n",
    "  colors = ['\\033[96m', '\\033[94m', '\\033[92m', '\\033[0m'] # green, blue, cyan, end\n",
    "  if type(word) == list:\n",
    "    return f\"{colors[color]}\"+\" \".join(word)+f\"{colors[-1]}\"\n",
    "  else:\n",
    "    return f\"{colors[color]}\"+word+f\"{colors[-1]}\"\n",
    "\n",
    "words = phrase_token_output['words']\n",
    "output = \"\"\n",
    "for start, end in phrase2word:\n",
    "  if end - start > 1:\n",
    "    output += color_words(words[start:end], start%3)\n",
    "  else:\n",
    "    output += \" \".join(words[start:end])\n",
    "  output += \" \"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdmmtBWUUKPs"
   },
   "source": [
    "### Add 1 to phrase_len `[MASK]`' to the target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1646681147324,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "-l5W_pCqUKPs"
   },
   "outputs": [],
   "source": [
    "phrase_masked_list = []\n",
    "word2char = phrase_token_output['word_offsets']\n",
    "\n",
    "mask_index_list = []\n",
    "mask_count = 0\n",
    "for p_s, p_e in phrase2word:\n",
    "    if p_e - p_s >= 2:\n",
    "        c_s = word2char[ p_s ][0]\n",
    "        c_e = word2char[ p_e - 1][1]\n",
    "        \n",
    "        mask_len = p_e - p_s\n",
    "        for l in range(1, mask_len+1):\n",
    "            phrase_masked_list.append(tgt_seq[0:c_s] + ' [MASK]' * l + ' ' + tgt_seq[c_e:])\n",
    "            mask_index_list.append([mask_count, mask_count + l])\n",
    "            mask_count += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1646681149469,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "LKVZQaXTUKPs",
    "outputId": "2d6e4113-50eb-4850-f0d6-13593501da4f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Disney has put out too many bland live remakes now and Beauty and the Beast is no exception.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1646681155036,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "p8mSIhZvUKPt",
    "outputId": "efabd3cf-331b-441c-9658-cfc143b5fdf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disney has \u001b[94m [MASK]\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has \u001b[94m [MASK]\u001b[0m\u001b[94m [MASK]\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n"
     ]
    }
   ],
   "source": [
    "phrase_masked_list\n",
    "for sent in phrase_masked_list:\n",
    "  print(sent.replace(f' {tokenizer.mask_token}', color_words(f' {tokenizer.mask_token}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3EvYRazUKPt"
   },
   "source": [
    "### Get masked token candidates from MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1646681167390,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "PcpNATQLUKPt"
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1646681174219,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "s_DUoAMZUKPt",
    "outputId": "10c6200e-fe73-4aa8-b436-6ed4fc49df76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 30522])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1646681177603,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "g56yvv0SUKPu"
   },
   "outputs": [],
   "source": [
    "mask_token_logits = torch.empty(len(mask_token_index), token_logits.shape[2])\n",
    "\n",
    "for i,ind in enumerate(mask_index_list):\n",
    "    li_s = mask_index_list[i][0]\n",
    "    li_e = mask_index_list[i][1]\n",
    "    ind_s = mask_token_index[li_s]\n",
    "    ind_e = mask_token_index[li_e - 1] + 1\n",
    "        \n",
    "    mask_token_logits[li_s:li_e] = token_logits[i, ind_s:ind_e, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1646681179812,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "KYEfm7CzUKPu",
    "outputId": "e2de3255-03d9-4f2c-b2b1-430e7c4cc781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=1).indices\n",
    "top_8_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT7IAx16UKPv"
   },
   "source": [
    "### Here get_substitutes check the combination of word candidates and rank them by perplexity (cross_entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1646681181435,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "b--ISR7zUKPv"
   },
   "outputs": [],
   "source": [
    "def get_substitutes(substitutes, tokenizer, mlm_model):\n",
    "    # all substitutes  list of list of token-id (all candidates)\n",
    "    c_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    word_list = []\n",
    "\n",
    "    # find all possible candidates \n",
    "    all_substitutes = []\n",
    "    for i in range(substitutes.size(0)):\n",
    "        if len(all_substitutes) == 0:\n",
    "            lev_i = substitutes[i]\n",
    "            all_substitutes = [[int(c)] for c in lev_i]\n",
    "        else:\n",
    "            lev_i = []\n",
    "            for all_sub in all_substitutes:\n",
    "                for j in substitutes[i]:\n",
    "                    lev_i.append(all_sub + [int(j)])\n",
    "            all_substitutes = lev_i\n",
    "\n",
    "    # all_substitutes = all_substitutes[:24]\n",
    "    all_substitutes = torch.tensor(all_substitutes) # [ N, L ]\n",
    "    all_substitutes = all_substitutes[:24].to(device)\n",
    "    \n",
    "    print(all_substitutes.shape) # (K ^ t, K)\n",
    "\n",
    "    N, L = all_substitutes.size()\n",
    "    word_predictions = mlm_model(all_substitutes)[0] # N L vocab-size\n",
    "    ppl = c_loss(word_predictions.view(N*L, -1), all_substitutes.view(-1)) # [ N*L ] \n",
    "    ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-1)) # N  \n",
    "    \n",
    "    _, word_list = torch.sort(ppl)\n",
    "    word_list = [all_substitutes[i] for i in word_list]\n",
    "    final_words = []\n",
    "    for word in word_list[:24]:\n",
    "        tokens = [tokenizer._convert_id_to_token(int(i)) for i in word]\n",
    "        text = tokenizer.convert_tokens_to_string(tokens)\n",
    "        final_words.append(text)\n",
    "        \n",
    "    del all_substitutes\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1474,
     "status": "ok",
     "timestamp": 1646681184320,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "jYzIoOQ2UKPw",
    "outputId": "0b981e1a-b685-4d98-9dc0-abc783408450",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n",
      "['done', 'made', 'seen', 'produced', 'created', 'had', 'released', 'got']\n",
      "Disney has  [MASK]  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mgot\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mmade\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mhad\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mdone\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mreleased\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "\n",
      "torch.Size([24, 2])\n",
      "['been', 'done', 'put', 'not', 'already', 'also', 'just', 'obviously']\n",
      "['far', 'doing', 'done', 'making', 'seen', 'made', 'producing', 'on']\n",
      "Disney has  [MASK] [MASK]  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mbeen on\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mput made\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mbeen producing\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mbeen done\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has  \u001b[94mput far\u001b[0m  too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i, (p_s, p_e)) in enumerate(mask_index_list):\n",
    "    cur_phrase = ''\n",
    "    substitutes = top_8_tokens[p_s:p_e]\n",
    "    final_words = get_substitutes(substitutes, tokenizer, mlm_model)\n",
    "    for s in substitutes:\n",
    "        print(tokenizer.convert_ids_to_tokens(s))\n",
    "    \n",
    "    print(phrase_masked_list[i])\n",
    "    for w in final_words[:5]:\n",
    "        print(phrase_masked_list[i].replace((f' {tokenizer.mask_token}' * (p_e - p_s))[1:], color_words(w)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2P7V6SyUKPw"
   },
   "source": [
    "## Importance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 2449,
     "status": "ok",
     "timestamp": 1646681208764,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "jNqk7Wj_UKPw"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "target_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/distilbert-base-uncased-imdb\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 3620,
     "status": "ok",
     "timestamp": 1646681212381,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "lSmA1AQZUKPw"
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1646681212630,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "kJ45KiyXUKPw"
   },
   "outputs": [],
   "source": [
    "# 1. retrieve logits and label from the target model\n",
    "inputs = tokenizer(entry['text'], return_tensors=\"pt\", truncation=True, max_length=512, return_token_type_ids=False)\n",
    "orig_logits = target_model(inputs['input_ids'].to(device), inputs['attention_mask'].to(device))[0].squeeze()\n",
    "orig_probs  = torch.softmax(orig_logits, -1)\n",
    "orig_label = torch.argmax(orig_probs)\n",
    "current_prob = orig_probs.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh2IBQFRUKPx"
   },
   "source": [
    "### Mask each phrase with `[UNK]` token and compute the confidence change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1646681218021,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "d5HYuzNVUKPx"
   },
   "outputs": [],
   "source": [
    "# return units masked with UNK at each position in the sequence\n",
    "def _get_unk_masked(units):\n",
    "    len_text = len(units)\n",
    "    masked_units = []\n",
    "    for i in range(len_text - 1):\n",
    "        masked_units.append(units[0:i] + ['[UNK]'] + units[i + 1:])\n",
    "    \n",
    "    # list of masked basic units\n",
    "    return masked_units\n",
    "\n",
    "'''\n",
    "input units should be phrase tokens\n",
    "'''\n",
    "def get_important_scores(units, tgt_model, orig_prob, orig_label, orig_probs, tokenizer, batch_size=8, max_length=512):\n",
    "    masked_units = _get_unk_masked(units)\n",
    "    texts = [' '.join(units) for units in masked_units]  # list of text of masked units\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(tokenizer)\n",
    "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_token_type_ids=False, return_tensors='pt')\n",
    "    \n",
    "    eval_data = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n",
    "    leave_1_probs = []\n",
    "    \n",
    "    tgt_model.eval() #make sure in inference stage\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch[0].to(device)      # input ids\n",
    "            attention_mask = batch[1].to(device) # attention mask\n",
    "        \n",
    "            leave_1_prob_batch = tgt_model(input_ids, attention_mask=attention_mask)[0]\n",
    "            leave_1_probs.append(leave_1_prob_batch)\n",
    "        \n",
    "    leave_1_probs = torch.cat(leave_1_probs, dim=0)  # words, num-label\n",
    "    leave_1_probs = torch.softmax(leave_1_probs, -1)\n",
    "    leave_1_probs_argmax = torch.argmax(leave_1_probs, dim=-1)\n",
    "    import_scores = (orig_prob\n",
    "                     - leave_1_probs[:, orig_label] # how the probability of original label decreases\n",
    "                     +\n",
    "                     (leave_1_probs_argmax != orig_label).float() # new label not equal to original label\n",
    "                     * (leave_1_probs.max(dim=-1)[0] - torch.index_select(orig_probs, 0, leave_1_probs_argmax))\n",
    "                     ).data.cpu().numpy()           # probability of changed label\n",
    "\n",
    "    return import_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16270,
     "status": "ok",
     "timestamp": 1646681236857,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "XRgZ1IqHUKPx",
    "outputId": "725aad44-c705-4b71-9467-211810009c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='bert-large-uncased-whole-word-masking', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "importance = get_important_scores(entry['phrases'], target_model, current_prob, orig_label, orig_probs, tokenizer, batch_size=8, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646681236858,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "SNNgPeLmUKPx",
    "outputId": "032839a4-8dcb-4aae-8b81-baa25d1f4cd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 1.129978),\n",
       " ('bland', 1.1032591),\n",
       " ('the', 1.0143657),\n",
       " ('beast', 0.71768385),\n",
       " ('exception', 0.62257254),\n",
       " ('is', 0.5944748),\n",
       " ('beauty', 0.52823466),\n",
       " ('and', 0.24859142),\n",
       " ('live', 0.2329458),\n",
       " ('no', 0.1979633),\n",
       " ('now', 0.008240223),\n",
       " ('remakes', -0.030231714),\n",
       " ('too', -0.08739191),\n",
       " ('many', -0.092213035),\n",
       " ('disney', -0.13981366),\n",
       " ('put out', -0.16544706),\n",
       " ('has', -0.19628477)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices = torch.argsort(torch.tensor(importance), dim=-1, descending=True).cpu()\n",
    "sorted_units = np.array(entry['phrases'])[sorted_indices]\n",
    "[(u,i) for (u,i) in zip(sorted_units, importance[sorted_indices])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiEbupbxUKPx"
   },
   "source": [
    "## Semantic Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_4aehacUKPy"
   },
   "source": [
    "### Mask the Word 'bland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1646681255307,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "nrNMIokaUKPy"
   },
   "outputs": [],
   "source": [
    "word = 'bland'\n",
    "word_start = tgt_seq.find(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1646681256440,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "jyY-BGmkUKPy",
    "outputId": "ac423b78-8369-4668-f63c-3c729d53ea4d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Disney has put out too many  [MASK]  live remakes now and Beauty and the Beast is no exception.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list = (tgt_seq[0:word_start] + ' [MASK] ' + tgt_seq[word_start+len(word):])\n",
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1646681259399,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "j5AxGzG7UKPy"
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1646681260012,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "HBo_WPl9UKPy",
    "outputId": "79835886-cf58-496a-fc62-146ea11d902b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disney has put out too many  [MASK]  live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mdisney\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94manimated\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mtraditionally\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94moriginal\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mclassic\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mepic\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mlimited\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mto\u001b[0m live remakes now and Beauty and the Beast is no exception.\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for t in tokenizer.convert_ids_to_tokens(top_8_tokens[0]):\n",
    "    print(phrase_masked_list.replace(f' {tokenizer.mask_token} ', color_words(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbNKIBHUUKPy"
   },
   "source": [
    "### No Mask - implicit semantic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1646681270705,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "TfWp01kRUKPz"
   },
   "outputs": [],
   "source": [
    "phrase_masked_list = tgt_seq\n",
    "\n",
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = tokenizer.convert_ids_to_tokens(inputs[0]).index(word)\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1646681274084,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "MMaZSPURUKPz",
    "outputId": "ffa81de5-1682-4d7c-a1c9-ed85f4fc8917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disney has put out too many bland live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mdisney\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94manimated\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94moriginal\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mlive\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mtraditionally\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mnew\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mlimited\u001b[0m live remakes now and Beauty and the Beast is no exception.\n",
      "Disney has put out too many \u001b[94mlong\u001b[0m live remakes now and Beauty and the Beast is no exception.\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for t in tokenizer.convert_ids_to_tokens(top_8_tokens):\n",
    "    print(phrase_masked_list.replace(word, color_words(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyUIH8gLUKPz"
   },
   "source": [
    "Observation:\n",
    "**Not deleting the word to be masked out does enforce semantic meaning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0DB80osUKPz"
   },
   "source": [
    "### What about multi-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1646677287139,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "JTOZBk71UKPz"
   },
   "outputs": [],
   "source": [
    "mask_token_index = torch.tensor([3,5])\n",
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_8_tokens = torch.topk(mask_token_logits, 8, dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1646677288026,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "1Gq71wKpUKPz",
    "outputId": "362ee153-6243-4333-f377-33e7428e83ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 2])\n"
     ]
    }
   ],
   "source": [
    "final_words = get_substitutes(top_8_tokens, tokenizer, mlm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646677288027,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "H7hFOlMbUKPz",
    "outputId": "5abb83eb-83ee-4302-890d-e008669af260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disney has put out too many bland sequals now and Frozen 2 is no exception.\n",
      "Disney has \u001b[94mput to\u001b[0m too many bland sequals now and Frozen 2 is no exception.\n",
      "Disney has \u001b[94mput a\u001b[0m too many bland sequals now and Frozen 2 is no exception.\n",
      "Disney has \u001b[94mput so\u001b[0m too many bland sequals now and Frozen 2 is no exception.\n",
      "Disney has \u001b[94msent so\u001b[0m too many bland sequals now and Frozen 2 is no exception.\n",
      "Disney has \u001b[94mput many\u001b[0m too many bland sequals now and Frozen 2 is no exception.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phrase_masked_list)\n",
    "for w in final_words[:5]:\n",
    "    print(phrase_masked_list.replace((f'put out'), color_words(w)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27W7nxs1UKP0"
   },
   "source": [
    "Observation:\n",
    "**Should not do this for phrases. Since it still enforces single-word semangtic meaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646677288028,
     "user": {
      "displayName": "Coraline Sun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16656428901050119206"
     },
     "user_tz": 480
    },
    "id": "rzGvHnqznMwL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (bert-attack)",
   "language": "python",
   "name": "bert-attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
